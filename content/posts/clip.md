---
title: "CLIPå›¾åƒæœç´¢å®ç°ä¸åŸç†"
date: 2024-06-24
draft: false
tags: ["CLIP", "LLM"]
categories: ["æŠ€æœ¯"]
---

> åœ¨å›¾åƒç›¸ä¼¼åº¦æ¯”è¾ƒçš„è¿‡ç¨‹ä¸­ï¼Œç¬”è€…æ„å¤–å‘ç°å¯ä»¥åˆ©ç”¨CLIPå¿«é€Ÿæ­å»ºä¸€ä¸ªç§æœ‰çš„å›¾åƒæœç´¢ç³»ç»Ÿã€‚æœ¬æ–‡å°†è¯¦ç»†ä»‹ç»å›¾åƒæœç´¢çš„å…³é”®å®ç°æ­¥éª¤ï¼Œå¹¶æ·±å…¥æ¢è®¨èƒŒåçš„åŸç†ï¼ŒåŒ…æ‹¬å‘é‡ç›¸ä¼¼åº¦ä¸æ£€ç´¢ä¼˜åŒ–ã€å‘é‡æ•°æ®åº“ã€Transformeræ¶æ„ä»¥åŠCLIPçš„æºä»£ç å®ç°ç­‰ã€‚


[TOC]



## ä¸€ æœ¬æ–‡å®ç°äº†ä»€ä¹ˆ

CLIP (Contrastive Languageâ€“Image Pre-training) æ˜¯ OpenAI å¼€å‘çš„å›¾æ–‡å¤šæ¨¡æ€æ¨¡å‹ ï¼Œå®ƒåˆ©ç”¨ä»äº’è”ç½‘æŠ“å–åˆ°çš„å›¾åƒ-æ–‡å­—pairæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨å¯¹æ¯”å­¦ä¹ çš„æ–¹å¼æ¥å­¦ä¹ å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰å¯¹åº”å…³ç³»ã€‚

ç¬”è€…å€ŸåŠ©CLIPæ¨¡å‹ï¼Œå®ç°äº†ä¸€ä¸ªç®€å•çš„å›¾åƒæœç´¢ç³»ç»Ÿã€‚è¿™æ˜¯æ¼”ç¤ºç¤ºä¾‹ï¼ŒåŒ…æ‹¬ä»¥æ–‡æœå›¾&ä»¥å›¾æœå›¾ï¼š

<div align="center">
<img src="/images/6/9fe5c722-682a-417b-99e4-8c7afe58efed.png" width=100% />
</div>

æœ¬æ–‡ä»‹ç»äº†å›¾åƒæœç´¢çš„å…³é”®å®ç°ã€‚å¹¶é¢å¤–æ¢ç©¶ã€è®°å½•äº†èƒŒåçš„ä¸€äº›åŸç†ï¼ŒåŒ…æ‹¬åŒ…æ‹¬å‘é‡ç›¸ä¼¼åº¦ä¸æ£€ç´¢ä¼˜åŒ–ã€å‘é‡æ•°æ®åº“ã€Transformerå’ŒCLIPæºä»£ç å®ç°ã€‚ã€‚

## äºŒ èƒŒæ™¯

### 1 ä»å›¾åƒç›¸ä¼¼åº¦æ¯”è¾ƒè¯´èµ·...

åœ¨è§†è§‰ç¨¿è½¬ä»£ç é¡¹ç›®ä¸­ï¼Œä¸å¯æˆ–ç¼ºæ˜¯å»ºç«‹ä¸€å¥—æ ‡å‡†ï¼Œé€šè¿‡æ¯”è¾ƒåŸè§†è§‰ç¨¿ä¸æ‰€ç”Ÿæˆä»£ç è§†è§‰ä¹‹é—´çš„å·®å¼‚ï¼Œæ¥ç¡®å®šè¿˜åŸæ•ˆæœã€‚è¿™é‡Œæœ‰ä¸€å¥—ç®€å•ã€å¿«é€Ÿçš„åƒç´ çº§å›¾åƒæ¯”è¾ƒæ–¹æ¡ˆï¼š[pixelmatch](https://github.com/mapbox/pixelmatch)ã€‚pixelmatch åªæœ‰**çº¦150 è¡Œä»£ç **ï¼Œæ²¡æœ‰ä¾èµ–å…³ç³»ï¼Œå¹¶ä¸”é€‚ç”¨äºåŸå§‹ç±»å‹çš„å›¾åƒæ•°æ®æ•°ç»„ã€‚

| Diff                           | expected                    | Actual                      |
| ------------------------------ | --------------------------- | --------------------------- |
|![å›¾ç‰‡](/images/6/1c207bfa-094f-4957-85be-a27dc3163576.png)  | ![å›¾ç‰‡](/images/6/11ad3536-3fc5-4da6-af42-c517bcb80963.png)        | ![å›¾ç‰‡](/images/6/dc1e79c4-e625-4de4-b161-1660be097645.png)   |
|![å›¾ç‰‡](/images/6/a7906867-5328-4b59-9a80-65d59433d853.png) | ![å›¾ç‰‡](/images/6/375b84ee-0630-4905-a14f-040e5c933a14.png) | ![å›¾ç‰‡](/images/6/552971f1-0fd8-4d72-8750-d86739c2a508.png) |


ä½†åœ¨ pixelmatch çš„ä½¿ç”¨è¿‡ç¨‹ä¸­ï¼Œå‘ç°å…¶å¯¹äºèƒŒæ™¯æ˜¯ä¸æ•æ„Ÿçš„ã€‚æ¯”å¦‚ä¸‹é¢è¿™ä¸ªç¤ºä¾‹ï¼Œpixelmatchå¯¹æ¯”ä¸¤ä¸ªå›¾ç‰‡çš„ç›¸ä¼¼åº¦ï¼Œé«˜è¾¾0.91ã€‚

è€Œåœ¨[Design2Code: How Far Are We From Automating Front-End Engineering?](https://huggingface.co/papers/2403.03163)è¿™ç¯‡è®ºæ–‡ä¸­ä»‹ç»åˆ°ï¼Œä»–ä»¬ä½¿ç”¨CLIPæ¥åˆ¤åˆ«ä¸¤å¼ å›¾ç‰‡åœ¨å®è§‚ä¸Šçš„å·®å¼‚æ€§ã€‚

> High-level Visual Similarity To evaluate the visual similarity of ImageR and ImageG, we use the similarity of their CLIP embedding, denoted as **CLIP**(ImageR, ImageG). Specifically, we extract features by CLIP-ViT-B/32 after resizing screenshots to squares. To rule out the texts in the screenshots, we use the inpainting algorithm from Telea to mask all detected text boxes using their bounding box coordinates.

å‚è€ƒä½¿ç”¨clipåï¼Œç›¸ä¼¼åº¦å°±ç¬¦åˆç›´è§‚é¢„æœŸåœ°æ¯”è¾ƒä½äº†ï¼š

<div align="center">
<img src="/images/6/5884beb7-8731-49d1-be00-820fe3b8335f.png" width=70% />
</div>


ä¸å¯æ€è®®çš„æ˜¯ï¼Œä½¿ç”¨CLIPæ—¶ï¼Œç¬”è€…çš„`calculate_similarity_score`çš„ä»£ç é‡å¤ªå°‘äº†ï¼Œæ ¸å¿ƒä»£ç åªæœ‰åå¤šè¡Œã€‚è¿™å¼•å‘äº†ç¬”è€…çš„å¥½å¥‡ï¼Œå¹¶æƒ³æ¢ç©¶ä¸€ä¸‹å…¶èƒŒåçš„åŸç†ã€‚

```python
import torch
import clip
from PIL import Image

device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

def calculate_similarity_score(img1_path, img2_path):
    image1_data = preprocess(Image.open(img1_path)).unsqueeze(0).to(device)
    image2_data = preprocess(Image.open(img2_path)).unsqueeze(0).to(device)
    # Calculate the embeddings for the images using the CLIP model
    with torch.no_grad():
        image1_embedding = model.encode_image(image1_data)
        image2_embedding = model.encode_image(image2_data)
    # Calculate the cosine similarity between the embeddings
    similarity_score = torch.nn.functional.cosine_similarity(image1_embedding, image2_embedding)
    return similarity_score.item()
```

### 2 å²‚ä¸æ˜¯å¯ä»¥å¿«é€Ÿæ„å»ºå›¾ç‰‡æœç´¢å¼•æ“?

ä¸¾ä¾‹æ¥è¯´ï¼Œæ¯”å¦‚æˆ‘ä»¬çš„æ‰‹æœºç›¸å†Œã€æˆ–èŠå¤©è®°å½•é‡Œï¼Œæœ‰è¿™æ ·çš„å‡ å¼ ç“·å™¨å›¾ç‰‡ã€‚

![å›¾ç‰‡](/images/6/379a1e07-f0a9-4df4-bc85-3b0f40ec0713.png)


ç»è¿‡ç¬”è€…çš„å°è¯•ï¼Œå‘ç° åä¸ºç›¸å†Œã€å°ç±³ç›¸å†Œã€QQæ¶ˆæ¯è®°å½•ã€å¾®ä¿¡æ¶ˆæ¯è®°å½• å…¶å®éƒ½æ˜¯æœä¸åˆ°è¿™äº›ç“·å™¨å›¾ç‰‡çš„ï¼š
![å›¾ç‰‡](/images/6/1b079be0-ac37-479a-a1ca-a2f4d75d59d2.png)


æ—¢ç„¶clipèƒ½å¤Ÿåšå›¾åƒçš„ç›¸ä¼¼åº¦æ¯”è¾ƒï¼Œå²‚ä¸æ˜¯å¯ä»¥ç”¨æ¥è½»æ˜“å®ç°å›¾åƒè¯­ä¹‰æœç´¢ï¼Ÿç®€å•åœ°å°è¯•ä¹‹åï¼Œç¬”è€…å°±ç”¨clipå®ç°äº†ä¸ªç®€å•çš„demoï¼Œæœç´¢â€œç“·å™¨â€çš„æ¼”ç¤ºç¤ºä¾‹å¦‚ä¸‹ï¼š


![å›¾ç‰‡](/images/6/d10654c6-1c2a-4632-92d5-17399bd392cd.png)


## ä¸‰ ä»£ç å®ç°

ç¬”è€…ä¸»è¦å€ŸåŠ©clip+Chroma+sqlite3+flaskå®ç°ã€‚

### 1 clip

è¿™ä¸€æ­¥çš„ä»£ç å®ç°æ¯”è¾ƒç®€å•ï¼Œä¸»è¦æ˜¯å€ŸåŠ©CLIPç›¸å…³æ¨¡å‹ï¼Œå°†æŒ‡å®šç›®å½•ä¸‹çš„å›¾åƒæ–‡ä»¶ï¼Œæ‰¹é‡ä¸€ä¸€è½¬ä¸ºå‘é‡æ•°æ®ã€‚

å€¼å¾—ä¸€æçš„æ˜¯ï¼Œè¿™é‡Œæœ€å¥½ä¸è¦ç›´æ¥ä½¿ç”¨OpenAI å¼€æºçš„ CLIP æ¨¡å‹ã€‚å› ä¸º OpenAI CLIPè®­ç»ƒç”¨çš„å›¾æ–‡æ•°æ®æ˜¯è‹±è¯­ä¸–ç•Œä¸­çš„å›¾åƒï¼Œè¿™å’Œä¸­æ–‡ä¸–ç•Œä¸­çš„å›¾åƒæœ‰å¾ˆå¤§çš„æ•°æ®åˆ†å¸ƒå·®å¼‚ï¼Œæ¶‰åŠåˆ°ä¸€äº›è­¬å¦‚æˆè¯­ã€ç”Ÿæ´»ä¹ ä¿—ã€åŸå¸‚åœ°ç‚¹æ—¶å¯èƒ½ä¼šå‡ºç°åå·®è¾ƒå¤§çš„æƒ…å†µã€‚

è¿™é‡Œç¬”è€…ä½¿ç”¨çš„æ˜¯WXGçš„WeCLIPï¼Œè¿™æ˜¯WXGè‡ªè¡Œè®­ç»ƒçš„ä¸€ä¸ªé€šç”¨çš„ä¸­æ–‡ CLIP æ¨¡å‹ã€‚å…·ä½“å¯ä»¥å‚çœ‹è¿™ç¯‡KMæ–‡ç« ï¼š[**WeCLIP**ï¼šå›¾æ–‡é¢„è®­ç»ƒæœ€å¼ºä¸­æ–‡CLIP æ¨¡å‹](https://km.woa.com/articles/show/592047)ã€‚å…¶ä½¿ç”¨èµ·æ¥ä¹Ÿæ¯”è¾ƒç®€å•ï¼Œå¦‚ä¸‹å³å¯åˆ†åˆ«å°†å›¾åƒã€æ–‡æœ¬æ•°æ®è½¬ä¸ºå‘é‡æ•°æ®ã€‚

```
import weclip
import torch

# usage: my_clip_model = MyClipModel('checkpoints/weclip_base.pth')
class MyClipModel:
    def __init__(self, clip_model_name):
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = weclip.WeCLIPBase()
        self.model.load_state_dict(torch.load(clip_model_name), strict=False)

    def image_embeddings(self, image_file_path):
        image_feature = self.model.forward_visual(image_file_path)
        image_embeddings = image_feature[0].tolist()
        return image_embeddings

    def text_embeddings(self, text):
        text_feature = self.model.forward_text(text)
        text_embeddings = text_feature[0].tolist()
        return text_embeddings
```

ä¹Ÿå¯ä»¥é€‰ç”¨å…¶ä»–çš„æ¨¡å‹ï¼Œæ¯”å¦‚ï¼š[Chinese-CLIP](https://github.com/OFA-Sys/Chinese-CLIP),  [openCLIP](https://github.com/mlfoundations/open_clip)ã€‚

### 2 å‘é‡æ•°æ®åº“

åœ¨ç¬¬äºŒç« èŠ‚ç¬¬1éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬æœ‰æåˆ°ä½¿ç”¨`torch.nn.functional.cosine_similarity`ç®€å•æ¯”è¾ƒä¸¤ä¸ªå›¾åƒé—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚ä½†æ˜¯å¦‚æœæ¶‰åŠåˆ°æ‰¹é‡çš„å›¾åƒæ•°æ®ï¼ŒæœŸæœ›æœç´¢æœ€ç›¸ä¼¼çš„topNå›¾åƒæ•°æ®æ—¶ï¼Œå‡ºäºæ€§èƒ½çš„è§’åº¦è€ƒè™‘ï¼Œæ˜¾ç„¶æˆ‘ä»¬ä¸èƒ½ç”¨æš´åŠ›æœç´¢çš„æ–¹å¼ï¼Œä¸€ä¸€å»åŒ¹é…ç›¸ä¼¼åº¦ã€å†æ’ä¸ªåºè¿”å›ã€‚

è¿™æ—¶ï¼Œæˆ‘ä»¬å°±éœ€è¦å‘é‡æ•°æ®åº“äº†ã€‚ä¸ä¼ ç»Ÿæ•°æ®åº“åŸºäºæ–‡æœ¬çš„ç²¾ç¡®åŒ¹é…ä¸åŒï¼Œå‘é‡æ•°æ®åº“æ˜¯ä»¥æ•°å­¦å½¢å¼å­˜å‚¨çš„å‘é‡æ•°æ®é›†åˆã€‚å‘é‡æ•°æ®åº“èƒ½å¤Ÿå°†å‘é‡å­˜å‚¨ä¸ºé«˜ç»´ç‚¹å¹¶è¿›è¡Œæ£€ç´¢ã€‚è¿™äº›æ•°æ®åº“å¢åŠ äº†é¢å¤–çš„åŠŸèƒ½ï¼Œå¯ä»¥é«˜æ•ˆã€å¿«é€Ÿåœ°æŸ¥æ‰¾ N ç»´ç©ºé—´ä¸­çš„æœ€è¿‘é‚»ã€‚è¿™äº›åŠŸèƒ½é€šå¸¸ç”± k æœ€è¿‘é‚»ï¼ˆk-NNï¼‰ç´¢å¼•æä¾›æ”¯æŒï¼Œå¹¶ä½¿ç”¨åˆ†å±‚å¯å¯¼èˆªå°ä¸–ç•Œï¼ˆHNSWï¼‰å’Œå€’æ’æ–‡ä»¶ç´¢å¼•ï¼ˆIVFï¼‰ç®—æ³•ç­‰ç®—æ³•æ„å»ºã€‚æ­¤å¤–ï¼Œå‘é‡æ•°æ®åº“å¯ä»¥æä¾›æ•°æ®ç®¡ç†ã€å®¹é”™ã€èº«ä»½éªŒè¯å’Œè®¿é—®æ§åˆ¶ä»¥åŠæŸ¥è¯¢å¼•æ“ç­‰å…¶ä»–åŠŸèƒ½ã€‚

é€šè¿‡å°†å›¾åƒç”Ÿæˆçš„å‘é‡ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“ä¸­ï¼Œæˆ‘ä»¬åœ¨åšæ£€ç´¢æ—¶ï¼Œå°±å¯ä»¥é€šè¿‡æŸ¥è¯¢ç›¸é‚»å‘é‡æ¥æ‰¾åˆ°ç›¸ä¼¼çš„ç›®æ ‡å›¾åƒäº†ã€‚è¿™é‡Œç¬”è€…ä½¿ç”¨çš„æ˜¯[Chroma](https://docs.trychroma.com/getting-started)ï¼Œå…¶ä½¿ç”¨èµ·æ¥æ¯”è¾ƒç®€å•ï¼Œæ–°å¢ã€æŸ¥è¯¢æ•°æ®å¦‚ä¸‹ç¤ºä¾‹ï¼š

```
# my_chroma_db = MyChromaDB(â€˜./data/my_image_chroma/â€™, 'my_images')
class MyChromaDB:
    def __init__(self, db_path, collection_name):
        # æŒ‡å®š Chroma çš„æ•°æ®åº“æ–‡ä»¶å­˜å‚¨è·¯å¾„ï¼Œå¦‚æœæ•°æ®å­˜åœ¨ï¼Œç¨‹åºå¯åŠ¨çš„æ—¶å€™ä¼šè‡ªåŠ¨åŠ è½½æ•°æ®åº“æ–‡ä»¶ã€‚
        self.client = chromadb.PersistentClient(path=db_path)
        self.collection = self.client.get_or_create_collection(name=collection_name)

    # æ–°å¢æ•°æ®
    def add_data(self, image: ImageRecord):
        self.collection.add(
            ids=[str(image.table_id)],
            embeddings=[image.embeddings],
            documents=[image.filename],
        )

    # æ ¹æ®idæŸ¥è¯¢æ•°æ®
    def get_by_id(self, image_id):
        return self.collection.get(ids=[image_id], include=["embeddings"])

    # æŸ¥è¯¢å‘é‡
    def query(self, embeddings, count_limit):
        return self.collection.query(query_embeddings=embeddings, n_results=count_limit)
```

### 3 web

webé¡µé¢è¿™é‡Œæ¯”è¾ƒç®€å•ï¼Œç¬”è€…ç”¨çš„flaskï¼Œæä¾›ä¸‹é¡µé¢ã€æŸ¥è¯¢æ¥å£ã€èµ„æºè®¿é—®æ¥å£å³å¯ã€‚è¿™é‡Œç®€å•åˆ—ä¸€ä¸‹æŸ¥è¯¢æ¥å£ï¼š

```
@app.post("/search")
def query():
    data = request.json
    print(f"query data{data}")
    image_res = []
    # ç”¨æˆ·æŸ¥è¯¢çš„å…³é”®è¯(searchTypeä¸ºtext)ï¼Œæˆ–è€…å›¾åƒçš„id(searchTypeä¸ºimage)
    keyword = str(data.get("keyword", ""))
    search_type = data.get("searchType", "text")
    if search_type == "image":
        chroma_image_record = my_chroma_db.get_by_id(keyword)
        embeddings = chroma_image_record["embeddings"]
    else:
        embeddings = my_clip_model.text_embeddings(keyword)
    embeddings_query_result = my_chroma_db.query(embeddings, NUM_IMAGE_RESULTS)  # {ids:[], distances:[]}
    image_ids = embeddings_query_result["ids"][0]
    distances = embeddings_query_result["distances"][0]
    for i in range(0, len(image_ids)):
        image_id = image_ids[i]
        distance = distances[i]
        image = my_sqlite3_db.query_by_id(image_id)
        if image is not None:
            image_res.append(convert_image_to_ui_data(image, distance))
    return generate_resp("", image_res)
```

## å›› æŠ€æœ¯åŸç†ä¹‹å‘é‡ç¯‡

### 1 å‘é‡ä¸ç‰¹å¾

åœ¨æ·±å…¥äº†è§£CLIPå‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆè¯¦ç»†äº†è§£ä¸€ä¸‹ç‰¹å¾å’Œå‘é‡çš„æ¦‚å¿µåŸç†ã€‚æˆ‘ä»¬å…ˆæ€è€ƒä¸€ä¸ªé—®é¢˜ï¼šåœ¨ç”Ÿæ´»ä¸­ï¼Œæˆ‘ä»¬æ˜¯å¦‚ä½•åŒºåˆ†å„ç§ç‰©ä½“ç§ç±»çš„ï¼Ÿ

å¦‚æœä»ç†è®ºè§’åº¦ä¸Šè®²ï¼Œè¿™æ˜¯å› ä¸ºæˆ‘ä»¬è¯†åˆ«ä¸åŒäº‹ç‰©ä¹‹é—´ä¸åŒçš„ç‰¹å¾ï¼Œè¿›è€Œèƒ½å¤Ÿè¯†åˆ«ç§ç±»ã€‚ä»¥åŒºåˆ†ä¸åŒç§ç±»çš„ç‹—å­ä¸ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç‹—å­çš„ä½“å‹å¤§å°ã€æ¯›å‘é•¿åº¦ã€é¼»å­é•¿çŸ­ç­‰ç‰¹å¾æ¥åŒºåˆ†ã€‚

![å›¾ç‰‡](/images/6/a200934c-2fd3-4f87-bc81-9ae45c501be7.png)


å¦‚è¿™å¼ ç…§ç‰‡æ‰€ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ç…§ä½“å‹åˆ†ç±»ï¼šç®€å•æŒ‰ç…§ä½“å‹å¤§å°æ’åºåï¼Œå¯ä»¥çœ‹åˆ°ä½“å‹è¶Šå¤§çš„ç‹—è¶Šé è¿‘åæ ‡è½´å³è¾¹ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±èƒ½å¾—åˆ°**ç‹—å­ä½“å‹ç‰¹å¾çš„ä¸€ç»´åæ ‡å’Œå¯¹åº”çš„æ•°å€¼**ï¼Œä» 0 åˆ° 1 çš„æ•°å­—ä¸­å¾—åˆ°æ¯åªç‹—åœ¨åæ ‡ç³»ä¸­çš„ä½ç½®ã€‚

ç„¶è€Œå•é ä¸€ä¸ªä½“å‹å¤§å°çš„ç‰¹å¾å¹¶ä¸å¤Ÿï¼Œåƒç…§ç‰‡ä¸­å“ˆå£«å¥‡ã€é‡‘æ¯›å’Œæ‹‰å¸ƒæ‹‰å¤šçš„ä½“å‹å°±éå¸¸æ¥è¿‘ï¼Œæˆ‘ä»¬æ— æ³•åŒºåˆ†ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¾èµ–ç»§ç»­è§‚å¯Ÿå…¶å®ƒç‰¹å¾ï¼Œä¾‹å¦‚æ¯›å‘çš„é•¿çŸ­ã€‚æˆ‘ä»¬å¯ä»¥é’ˆå¯¹æ¯›å‘çš„é•¿çŸ­ï¼Œå»ºç«‹ç¬¬äºŒçš„ç»´åº¦ï¼Œè¿™æ ·æ¯åªç‹—å¯¹åº”ä¸€ä¸ªäºŒç»´åæ ‡ç‚¹ï¼Œæˆ‘ä»¬å°±èƒ½è½»æ˜“çš„å°†å“ˆå£«å¥‡ã€é‡‘æ¯›å’Œæ‹‰å¸ƒæ‹‰å¤šåŒºåˆ†å¼€æ¥ã€‚

ä½†æ˜¯ä»…é ä½“å‹ã€æ¯›å‘é•¿çŸ­ï¼Œè¿˜æ˜¯ä¸å¤Ÿï¼šè¿™æ—¶ä»ç„¶æ— æ³•å¾ˆå¥½çš„åŒºåˆ†å¾·ç‰§å’Œç½—å¨çº³çŠ¬ã€‚æˆ‘ä»¬å¯ä»¥ç»§ç»­å†ä»å…¶å®ƒçš„ç‰¹å¾åŒºåˆ†ï¼Œæ¯”å¦‚é¼»å­çš„é•¿çŸ­ï¼Œè¿™æ ·å°±èƒ½å¾—åˆ°ä¸€ä¸ªä¸‰ç»´çš„åæ ‡ç³»å’Œæ¯åªç‹—åœ¨ä¸‰ç»´åæ ‡ç³»ä¸­çš„ä½ç½®ã€‚å¦‚æ­¤å¾€å¤ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåªè¦ç‰¹å¾è¶³å¤Ÿå¤šï¼Œå°±èƒ½å¤Ÿå°†æ‰€æœ‰çš„ç‹—åŒºåˆ†å¼€æ¥ï¼Œæœ€åå°±èƒ½å¾—åˆ°ä¸€ä¸ªé«˜ç»´çš„åæ ‡ç³»ï¼Œä¹Ÿè®¸æˆ‘ä»¬æƒ³è±¡ä¸å‡ºé«˜ç»´åæ ‡ç³»é•¿ä»€ä¹ˆæ ·ï¼Œä½†æ˜¯åœ¨æ•°ç»„ä¸­ï¼Œæˆ‘ä»¬åªéœ€è¦ä¸€ç›´å‘æ•°ç»„ä¸­è¿½åŠ æ•°å­—å°±å¯ä»¥äº†ã€‚

å®é™…ä¸Šï¼Œåªè¦ç»´åº¦å¤Ÿå¤šï¼Œæˆ‘ä»¬å°±èƒ½å¤Ÿå°†æ‰€æœ‰çš„äº‹ç‰©åŒºåˆ†å¼€æ¥ï¼šä»å…·ä½“çš„å±±æ²³æ—¥æœˆã€é¸Ÿå…½é±¼è™«ï¼Œåˆ°æŠ½è±¡çš„å–œæ€’å“€ä¹ã€æ‚²æ¬¢ç¦»åˆï¼Œä¸–é—´ä¸‡ç‰©éƒ½å¯ä»¥ç”¨ä¸€ä¸ªå¤šç»´åæ ‡ç³»æ¥è¡¨ç¤ºï¼Œå®ƒä»¬éƒ½åœ¨ä¸€ä¸ªé«˜ç»´çš„ç‰¹å¾ç©ºé—´ä¸­å¯¹åº”ç€ä¸€ä¸ªåæ ‡ç‚¹ã€‚
![å›¾ç‰‡](/images/6/304d61da-6f35-4536-9628-e2a512d24c26.png)


è¿™æ ·ï¼Œæˆ‘ä»¬èƒ½ç”¨å‘é‡è¡¨ç¤ºä¸–é—´ä¸‡ç‰©çš„ç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾ï¼ˆç‰¹å¾ä¸ä¸€å®šæ˜¯ç‹—å­èº«é«˜é‚£ç§å…·ä½“çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯æŠ½è±¡çš„ï¼‰å°±æ˜¯ä¸€ä¸ªå‘é‡çš„ç»´åº¦ã€‚

### 2 ç›¸ä¼¼å‘é‡è®¡ç®—

å’Œä¼ ç»Ÿçš„æ•°æ®åŒ¹é…ä¸åŒï¼Œå‘é‡æ•°æ®çš„æŸ¥è¯¢ï¼Œå¤šæ˜¯æŸ¥è¯¢ä¸æŒ‡å®šå¤šç»´åæ ‡ä½ç½®ï¼ˆæŒ‡å®šå‘é‡ï¼‰æ¥è¿‘çš„å‘é‡ï¼Œæ˜¯æ¨¡ç³Šçš„ç›¸ä¼¼æ€§è®¡ç®—ã€‚ä¾‹å¦‚æˆ‘ä»¬æŸ¥è¯¢ä¸€åªç‹—å­å…·ä½“æ˜¯ä»€ä¹ˆå“ç§ï¼Œæˆ‘ä»¬çŸ¥é“äº†å®ƒçš„ä½“å‹ã€æ¯›å‘ã€é¼»é•¿ã€çœ¼ç›å¤§å°ç­‰ç­‰ç‰¹å¾ï¼Œæˆ‘ä»¬å…ˆå¾—åˆ°è¯¥ç‹—å­çš„ç‰¹å®šå‘é‡ï¼Œç„¶åè¦åœ¨ä¼—å¤šå‘é‡ä¸­æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ä¸€äº›ã€‚æˆ‘ä»¬æ ¹æ®ç›¸ä¼¼å‘é‡çš„ç‹—å­å“ç§ï¼Œæ¥ç¡®å®šè¯¥ç‹—å­æœ€ç»ˆçš„å“ç§ã€‚

#### 2.1 ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆCosine Similarityï¼‰

ä½™å¼¦ç›¸ä¼¼åº¦æ˜¯æŒ‡ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„å¤¹è§’ä½™å¼¦å€¼ï¼Œå®ƒçš„è®¡ç®—å…¬å¼å¦‚å›¾æ‰€ç¤ºã€‚å…¶ä¸­ï¼Œ`A` å’Œ` B` åˆ†åˆ«è¡¨ç¤ºä¸¤ä¸ªå‘é‡ï¼Œ`Â·`è¡¨ç¤ºå‘é‡çš„ç‚¹ç§¯ï¼Œ`|A|`å’Œ`|B|`åˆ†åˆ«è¡¨ç¤ºä¸¤ä¸ªå‘é‡çš„æ¨¡é•¿ã€‚è‹¥ä¸¤ä¸ªå‘é‡ä¹‹é—´å¤¹è§’è¶Šå°ï¼Œåˆ™å‘é‡è¶Šç›¸ä¼¼ã€‚

ä½™å¼¦ç›¸ä¼¼åº¦å¯¹å‘é‡çš„é•¿åº¦ä¸æ•æ„Ÿï¼Œåªå…³æ³¨å‘é‡çš„æ–¹å‘ï¼Œæ¯”è¾ƒé€‚ç”¨äºé«˜ç»´å‘é‡çš„ç›¸ä¼¼æ€§è®¡ç®—ã€‚ä¾‹å¦‚è¯­ä¹‰æœç´¢å’Œæ–‡æ¡£åˆ†ç±»ã€‚

<div align="center">
<img src="/images/6/5f7f4ee1-90cf-4562-846f-39d8ad58e9b1.png" width=30% />
</div>


#### 2.2 æ¬§å‡ é‡Œå¾—è·ç¦»ï¼ˆEuclidean Distanceï¼‰

æ¬§å‡ é‡Œå¾—è·ç¦»æ˜¯æŒ‡ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„è·ç¦»ï¼Œå®ƒçš„è®¡ç®—å…¬å¼å¦‚å›¾æ‰€ç¤ºã€‚å…¶ä¸­ï¼Œ`A` å’Œ` B` åˆ†åˆ«è¡¨ç¤ºä¸¤ä¸ªå‘é‡ï¼Œ`n` è¡¨ç¤ºå‘é‡çš„ç»´åº¦ã€‚æ¬§å¼è·ç¦»è¶Šå°è¡¨ç¤ºå‘é‡çš„é¡¶ç‚¹ä¹‹é—´è¶Šè¿‘ï¼Œå³å‘é‡ä¹‹é—´æ›´ç›¸ä¼¼ã€‚

æ¬§å‡ é‡Œå¾—è·ç¦»ç®—æ³•çš„ä¼˜ç‚¹æ˜¯å¯ä»¥åæ˜ å‘é‡çš„ç»å¯¹è·ç¦»ï¼Œé€‚ç”¨äºéœ€è¦è€ƒè™‘å‘é‡é•¿åº¦çš„ç›¸ä¼¼æ€§è®¡ç®—ã€‚ä¾‹å¦‚æ¨èç³»ç»Ÿä¸­ï¼Œéœ€è¦æ ¹æ®ç”¨æˆ·çš„å†å²è¡Œä¸ºæ¥æ¨èç›¸ä¼¼çš„å•†å“ï¼Œè¿™æ—¶å°±éœ€è¦è€ƒè™‘ç”¨æˆ·çš„å†å²è¡Œä¸ºçš„æ•°é‡ï¼Œè€Œä¸ä»…ä»…æ˜¯ç”¨æˆ·çš„å†å²è¡Œä¸ºçš„ç›¸ä¼¼åº¦ã€‚

<div align="center">
<img src="/images/6/5730cfa8-5235-4bd7-857a-a676ee2a7c50.png" width=30% />
</div>


#### 2.3 ç‚¹ç§¯ç›¸ä¼¼åº¦ (Dot product Similarity)

ä½™å¼¦ç›¸ä¼¼åº¦æ˜¯æŒ‡ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„å¤¹è§’ä½™å¼¦å€¼ï¼Œå®ƒçš„è®¡ç®—å…¬å¼å¦‚å›¾æ‰€ç¤ºã€‚å…¶ä¸­ï¼Œ`A` å’Œ` B` åˆ†åˆ«è¡¨ç¤ºä¸¤ä¸ªå‘é‡ï¼Œ`n` è¡¨ç¤ºå‘é‡çš„ç»´åº¦ã€‚

ç‚¹ç§¯ç›¸ä¼¼åº¦ç®—æ³•çš„ä¼˜ç‚¹åœ¨äºå®ƒç®€å•æ˜“æ‡‚ï¼Œè®¡ç®—é€Ÿåº¦å¿«ï¼Œå¹¶ä¸”å…¼é¡¾äº†å‘é‡çš„é•¿åº¦å’Œæ–¹å‘ã€‚å®ƒé€‚ç”¨äºè®¸å¤šå®é™…åœºæ™¯ï¼Œä¾‹å¦‚å›¾åƒè¯†åˆ«ã€è¯­ä¹‰æœç´¢å’Œæ–‡æ¡£åˆ†ç±»ç­‰ã€‚ä½†ç‚¹ç§¯ç›¸ä¼¼åº¦ç®—æ³•å¯¹å‘é‡çš„é•¿åº¦æ•æ„Ÿï¼Œå› æ­¤åœ¨è®¡ç®—é«˜ç»´å‘é‡çš„ç›¸ä¼¼æ€§æ—¶å¯èƒ½ä¼šå‡ºç°é—®é¢˜ã€‚

<div align="center">
<img src="/images/6/1aabc1d3-384f-4d52-96c5-eb44f7bfe315.png" width=30% />
</div>


#### 2.4 æµ·æ˜è·ç¦»

ä¸¥æ ¼æ¥è¯´ï¼Œæµ·æ˜è·ç¦»å…¶å®å’Œå‘é‡æ²¡æœ‰å¤ªå¤§å…³ç³»ï¼Œæµ·æ˜è·ç¦»è®¡ç®—çš„æ˜¯ä¸¤ä¸ªç­‰é•¿å­—ç¬¦ä¸²å¯¹åº”ä½ç½®å­—ç¬¦ä¸åŒçš„ä¸ªæ•°ã€‚

å¯¹äºå‘é‡æ¥è¯´ï¼Œæµ·æ˜è·ç¦»å¯ä»¥çœ‹ä½œæ˜¯å°†ä¸€ä¸ªå‘é‡å˜æ¢æˆå¦ä¸€ä¸ªå‘é‡æ‰€éœ€è¦æ›¿æ¢çš„åæ ‡ä¸ªæ•°ã€‚

<div align="center">
<img src="/images/6/382ef521-cf20-403f-9fa9-95a3d50f53cc.png" width=40% />
</div>


ä¸åŒçš„è·ç¦»è®¡ç®—å…¬å¼ååº”çš„æ˜¯å‘é‡ä¸åŒç»´åº¦çš„ç‰¹å¾ï¼Œéƒ½æœ‰å…¶ä¼˜ç‚¹å’Œç¼ºç‚¹ï¼Œéœ€è¦å¼€å‘è€…æ ¹æ®è‡ªå·±çš„æ•°æ®ç‰¹å¾å’Œä¸šåŠ¡åœºæ™¯æ¥é€‰æ‹©ã€‚å…·ä½“åœ°ï¼Œå¼€å‘äººå‘˜å¯ä»¥åˆ†åˆ«ç”¨å‡ ç§å…¬å¼è¿›è¡Œå¬å›ç‡æµ‹è¯•ï¼Œçœ‹å“ªä¸ªå¬å›ç‡æ›´é«˜ä¸€äº›ä»€ä¹ˆçš„ã€‚

### 3 å‘é‡æ£€ç´¢

çŸ¥é“äº†å‘é‡ç›¸ä¼¼æ€§çš„æ¯”è¾ƒæ–¹æ³•ï¼ˆå¯ä»¥é€šè¿‡æ¯”è¾ƒå‘é‡ä¹‹é—´çš„è·ç¦»æ¥åˆ¤æ–­å®ƒä»¬çš„ç›¸ä¼¼åº¦ï¼‰ï¼Œæˆ‘ä»¬è¿˜è¦å¯¹å‘é‡æŸ¥è¯¢æœ‰æ–¹æ³•è®ºã€‚ä¸ç„¶çš„è¯ï¼Œå¦‚æœæƒ³è¦åœ¨ä¸€ä¸ªæµ·é‡çš„æ•°æ®ä¸­æ‰¾åˆ°å’ŒæŸä¸ªå‘é‡æœ€ç›¸ä¼¼çš„å‘é‡ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®åº“ä¸­çš„æ¯ä¸ªå‘é‡è¿›è¡Œä¸€æ¬¡æ¯”è¾ƒè®¡ç®—ã€‚è¿™æ ·çš„è®¡ç®—é‡æ˜¯éå¸¸å·¨å¤§çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸€ç§é«˜æ•ˆçš„ç®—æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

é«˜æ•ˆçš„æœç´¢ç®—æ³•æœ‰å¾ˆå¤šï¼Œå…¶ä¸»è¦æ€æƒ³æ˜¯é€šè¿‡ä¸¤ç§æ–¹å¼æé«˜æœç´¢æ•ˆç‡ï¼š

1. å‡å°‘å‘é‡å¤§å°â€”â€”é€šè¿‡é™ç»´æˆ–å‡å°‘è¡¨ç¤ºå‘é‡å€¼çš„é•¿åº¦ã€‚
2. ç¼©å°æœç´¢èŒƒå›´â€”â€”å¯ä»¥é€šè¿‡èšç±»æˆ–å°†å‘é‡ç»„ç»‡æˆåŸºäºæ ‘å½¢ã€å›¾å½¢ç»“æ„æ¥å®ç°ï¼Œå¹¶é™åˆ¶æœç´¢èŒƒå›´ä»…åœ¨æœ€æ¥è¿‘çš„ç°‡ä¸­è¿›è¡Œï¼Œæˆ–è€…é€šè¿‡æœ€ç›¸ä¼¼çš„åˆ†æ”¯è¿›è¡Œè¿‡æ»¤ã€‚

å¯¹å‘é‡æ£€ç´¢æ¥è¯´ï¼Œé€šå¸¸æœ‰ä¸‰ç±»æ–¹æ³•ï¼šåŸºäºæ ‘çš„æ–¹æ³•ã€Hashæ–¹æ³•ã€çŸ¢é‡é‡åŒ–æ–¹æ³•ã€‚

#### 3.1 åŸºäºæ ‘ï¼ˆTree-based methodsï¼‰

è¿™ç±»æ–¹æ³•é€šè¿‡æ„å»ºæ ‘ç»“æ„æ¥ç»„ç»‡æ•°æ®ï¼Œä»¥ä¾¿å¿«é€Ÿæ£€ç´¢ã€‚ä¾‹å¦‚ï¼ŒKDæ ‘ï¼ˆk-dimensional treeï¼‰å’ŒRæ ‘ï¼ˆR-treeï¼‰æ˜¯ä¸¤ç§å¸¸è§çš„åŸºäºæ ‘çš„æ•°æ®ç»“æ„ï¼Œå®ƒä»¬å¯ä»¥ç”¨æ¥å­˜å‚¨ç©ºé—´æ•°æ®ï¼Œä»¥ä¾¿è¿›è¡Œå¿«é€Ÿçš„è¿‘é‚»æœç´¢ã€‚åŸºäºæ ‘çš„æ–¹æ³•é€‚åˆäºç»´åº¦ä¸æ˜¯ç‰¹åˆ«é«˜çš„æƒ…å†µï¼Œå› ä¸ºåœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œæ ‘ç»“æ„çš„æ•ˆç‡ä¼šå› ä¸ºâ€œç»´åº¦çš„è¯…å’’â€è€Œå¤§å¹…ä¸‹é™ã€‚

#### 3.2 å“ˆå¸Œæ–¹æ³•ï¼ˆHashing methodsï¼‰

å“ˆå¸Œæ–¹æ³•é€šè¿‡å“ˆå¸Œå‡½æ•°å°†é«˜ç»´å‘é‡æ˜ å°„åˆ°ä½ç»´çš„å“ˆå¸Œç ä¸Šï¼Œä½¿å¾—ç›¸ä¼¼çš„æ•°æ®ç‚¹åœ¨å“ˆå¸Œç©ºé—´ä¸­ä¹Ÿç›¸é‚»ã€‚å±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼ˆLocality-Sensitive Hashing, LSHï¼‰æ˜¯ä¸€ç§è‘—åçš„å“ˆå¸Œæ–¹æ³•ï¼Œå®ƒèƒ½å¤Ÿä¿è¯åœ¨é«˜ç»´ç©ºé—´ä¸­è·ç¦»ç›¸è¿‘çš„ç‚¹åœ¨å“ˆå¸Œåçš„ä½ç»´ç©ºé—´ä¸­ä»ç„¶ç›¸è¿‘ã€‚å“ˆå¸Œæ–¹æ³•é€šå¸¸é€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†å’Œé«˜ç»´æ•°æ®çš„å¿«é€Ÿæ£€ç´¢ã€‚

#### 3.3 çŸ¢é‡é‡åŒ–æ–¹æ³•ï¼ˆVector Quantization methods)

çŸ¢é‡é‡åŒ–æ˜¯ä¸€ç§é€šè¿‡å°†å‘é‡ç©ºé—´åˆ’åˆ†ä¸ºæœ‰é™æ•°é‡çš„åŒºåŸŸï¼Œå¹¶ç”¨è¿™äº›åŒºåŸŸçš„ä»£è¡¨ç‚¹ï¼ˆè´¨å¿ƒï¼‰æ¥è¿‘ä¼¼è¡¨ç¤ºæ‰€æœ‰çš„ç‚¹çš„æ–¹æ³•ã€‚æœ€è‘—åçš„çŸ¢é‡é‡åŒ–æ–¹æ³•æ˜¯kå‡å€¼ç®—æ³•ï¼ˆk-meansï¼‰ï¼Œå®ƒå°†æ•°æ®ç‚¹èšç±»æˆkä¸ªç°‡ï¼Œå¹¶ç”¨ç°‡çš„è´¨å¿ƒæ¥ä»£è¡¨ç°‡ä¸­çš„æ‰€æœ‰ç‚¹ã€‚ä¹˜ç§¯é‡åŒ–ï¼ˆProduct Quantizationï¼‰å’Œä¼˜åŒ–ä¹˜ç§¯é‡åŒ–ï¼ˆOptimized Product Quantizationï¼‰æ˜¯ä¸¤ç§ç”¨äºå¤§è§„æ¨¡ç›¸ä¼¼æ€§æœç´¢çš„çŸ¢é‡é‡åŒ–æŠ€æœ¯ã€‚çŸ¢é‡é‡åŒ–æ–¹æ³•é€‚ç”¨äºéœ€è¦å‹ç¼©æ•°æ®ä»¥èŠ‚çœå­˜å‚¨ç©ºé—´å’ŒåŠ é€Ÿæ£€ç´¢çš„åœºæ™¯ã€‚

##### 3.3.1 kå‡å€¼ç®—æ³•ï¼ˆk-meansï¼‰

æˆ‘ä»¬å¯ä»¥åœ¨ä¿å­˜å‘é‡æ•°æ®åï¼Œå…ˆå¯¹å‘é‡æ•°æ®å…ˆè¿›è¡Œèšç±»ã€‚

å¸¸è§çš„èšç±»ç®—æ³•æœ‰ K-Meansï¼Œå®ƒå¯ä»¥å°†æ•°æ®åˆ†æˆ k ä¸ªç±»åˆ«ï¼Œå…¶ä¸­ k æ˜¯é¢„å…ˆæŒ‡å®šçš„ã€‚ä»¥ä¸‹æ˜¯ k-means ç®—æ³•çš„åŸºæœ¬æ­¥éª¤ï¼š

1. é€‰æ‹© k ä¸ªåˆå§‹èšç±»ä¸­å¿ƒã€‚

2. å°†æ¯ä¸ªæ•°æ®ç‚¹åˆ†é…åˆ°æœ€è¿‘çš„èšç±»ä¸­å¿ƒã€‚

3. è®¡ç®—æ¯ä¸ªèšç±»çš„æ–°ä¸­å¿ƒã€‚

4. é‡å¤æ­¥éª¤ 2 å’Œ 3ï¼Œç›´åˆ°èšç±»ä¸­å¿ƒä¸å†æ”¹å˜æˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚

![å›¾ç‰‡](/images/6/ad254d5f-e519-4751-bbbf-661030d8a215.png)


ä½†æ˜¯è¿™ç§æœç´¢æ–¹å¼ä¹Ÿæœ‰ä¸€äº›ç¼ºç‚¹ï¼Œä¾‹å¦‚åœ¨æœç´¢çš„æ—¶å€™ï¼Œå¦‚æœæœç´¢çš„å†…å®¹æ­£å¥½å¤„äºä¸¤ä¸ªåˆ†ç±»åŒºåŸŸçš„ä¸­é—´ï¼Œå°±å¾ˆæœ‰å¯èƒ½é—æ¼æ‰æœ€ç›¸ä¼¼çš„å‘é‡ã€‚

##### 3.3.2 ä¹˜ç§¯é‡åŒ–ï¼ˆProduct Quantizationï¼‰

è¿™é‡Œçš„ä¹˜ç§¯æ˜¯æŒ‡ç¬›å¡å°”ç§¯ï¼ˆCartesian Productï¼‰ï¼Œæ„æ€æ˜¯æŒ‡æŠŠåŸæ¥çš„å‘é‡ç©ºé—´åˆ†è§£ä¸ºè‹¥å¹²ä¸ªä½ç»´å‘é‡ç©ºé—´çš„ç¬›å¡å°”ç§¯ï¼Œå¹¶å¯¹åˆ†è§£å¾—åˆ°çš„ä½ç»´å‘é‡ç©ºé—´åˆ†åˆ«åšé‡åŒ–ï¼ˆQuantizationï¼‰ã€‚è¿™æ ·æ¯ä¸ªå‘é‡å°±èƒ½ç”±å¤šä¸ªä½ç»´ç©ºé—´çš„é‡åŒ–codeç»„åˆè¡¨ç¤ºã€‚

PQæ˜¯ä¸€ç§é‡åŒ–ï¼ˆquantizationï¼‰æ–¹æ³•ï¼Œæœ¬è´¨ä¸Šæ˜¯æ•°æ®çš„ä¸€ç§å‹ç¼©è¡¨è¾¾æ–¹æ³•ï¼Œæ‰€ä»¥è¯¥æ–¹æ³•é™¤äº†å¯ä»¥ç”¨åœ¨ç›¸ä¼¼æœç´¢å¤–ï¼Œè¿˜å¯ä»¥ç”¨äºæ¨¡å‹å‹ç¼©ï¼Œç‰¹åˆ«æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œçš„æ¨¡å‹å‹ç¼©ä¸Šã€‚

![å›¾ç‰‡](/images/6/30cf7d98-2dfb-49f9-a636-288b6f0bd2ba.png)


å¦‚å›¾æ‰€ç¤ºï¼Œä¸€ä¸ª128ç»´çš„å‘é‡ï¼Œå¯ä»¥åˆ†å‰²ä¸º8ä¸ª16ç»´çš„ä½çº¬ç»´åº¦å­å‘é‡ï¼Œå¹¶åˆ†åˆ«å¯¹è¿™8ä¸ª16ç»´çš„ä½ç»´å‘é‡ç‹¬ç«‹ç”¨kmeansç®—æ³•åšé‡åŒ–ã€‚åœ¨å›¾ä¸­ï¼Œå‡è®¾16ç»´çš„ä½ç»´å­å‘é‡éœ€è¦256ä¸ªèšç±»ä¸­å¿ƒï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ª0ï½255çš„å€¼å»æ˜ å°„ã€‚è¿™æ ·ï¼ŒåŸå…ˆä¸€ä¸ª128ç»´çš„å‘é‡ï¼Œå°±å¯ä»¥ç”¨8ä¸ª0ï½255çš„å€¼ï¼Œè¡¨ç¤ºåŸå§‹128ç»´å‘é‡æœ€ç»ˆçš„é‡åŒ–ç¼–ç å€¼ã€‚

##### 3.3.3 åˆ†å±‚å¯¼èˆªå°ä¸–ç•Œï¼ˆHierarchical Navigable Small Worldsï¼‰ (HNSW)

åˆ†å±‚å¯¼èˆªå°ä¸–ç•Œï¼ˆLayered Navigation Small Worldï¼‰æ˜¯ä¸€ç§ç”¨äºæå‡ç”¨æˆ·åœ¨å¤æ‚ç¯å¢ƒæˆ–å¤§é‡ä¿¡æ¯ä¸­å¯»æ‰¾ç›®æ ‡å†…å®¹æ—¶çš„å¯¼èˆªæ•ˆç‡å’Œä½“éªŒçš„è®¾è®¡æ–¹æ³•ã€‚å®ƒå€Ÿé‰´äº†â€œå°ä¸–ç•Œç½‘ç»œâ€ï¼ˆSmall World Networkï¼‰çš„æ¦‚å¿µï¼Œé€šè¿‡æ„å»ºå¤šå±‚çº§çš„å¯¼èˆªç»“æ„ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿåœ¨æœ‰é™çš„æ—¶é—´å†…å¿«é€Ÿæ‰¾åˆ°æ‰€éœ€ä¿¡æ¯ã€‚å› ä¸ºä¸å¥½è¿›è¡Œæ–‡å­—ä¸Šçš„æè¿°ï¼Œè¿™é‡Œæ¨èè¿™æ­Œè§†é¢‘ [HNSW for Vector Search Explained and Implemented with Faiss (Python)](https://www.youtube.com/watch?v=QvKMwLjdK-s) åŠå…¶åšå®¢ [HNSW](https://www.pinecone.io/learn/series/faiss/hnsw/)

![å›¾ç‰‡](/images/6/12c79caf-47f5-4648-af51-f80744be96c6.png)


### 4 è¿‡æ»¤ (Filtering)

åœ¨å®é™…çš„ä¸šåŠ¡åœºæ™¯ä¸­ï¼Œå¾€å¾€ä¸éœ€è¦åœ¨æ•´ä¸ªå‘é‡æ•°æ®åº“ä¸­è¿›è¡Œç›¸ä¼¼æ€§æœç´¢ï¼Œè€Œæ˜¯é€šè¿‡éƒ¨åˆ†çš„ä¸šåŠ¡å­—æ®µè¿›è¡Œè¿‡æ»¤å†è¿›è¡ŒæŸ¥è¯¢ã€‚æ‰€ä»¥å­˜å‚¨åœ¨æ•°æ®åº“çš„å‘é‡å¾€å¾€è¿˜éœ€è¦åŒ…å«å…ƒæ•°æ®ï¼Œä¾‹å¦‚ç”¨æˆ· IDã€åŸå¸‚IDç­‰ä¿¡æ¯ã€‚è¿™æ ·å°±å¯ä»¥åœ¨æœç´¢çš„æ—¶å€™ï¼Œæ ¹æ®å…ƒæ•°æ®æ¥è¿‡æ»¤æœç´¢ç»“æœï¼Œä»è€Œå¾—åˆ°æœ€ç»ˆçš„ç»“æœã€‚

ä¸ºæ­¤ï¼Œå‘é‡æ•°æ®åº“é€šå¸¸ç»´æŠ¤ä¸¤ä¸ªç´¢å¼•ï¼šä¸€ä¸ªæ˜¯å‘é‡ç´¢å¼•ï¼Œå¦ä¸€ä¸ªæ˜¯å…ƒæ•°æ®ç´¢å¼•ã€‚ç„¶åï¼Œåœ¨è¿›è¡Œç›¸ä¼¼æ€§æœç´¢æœ¬èº«ä¹‹å‰æˆ–ä¹‹åæ‰§è¡Œå…ƒæ•°æ®è¿‡æ»¤ï¼Œä½†æ— è®ºå“ªç§æƒ…å†µä¸‹ï¼Œéƒ½å­˜åœ¨å¯¼è‡´æŸ¥è¯¢è¿‡ç¨‹å˜æ…¢çš„å›°éš¾ã€‚

è¿‡æ»¤è¿‡ç¨‹å¯ä»¥åœ¨å‘é‡æœç´¢æœ¬èº«ä¹‹å‰æˆ–ä¹‹åæ‰§è¡Œï¼Œä½†æ¯ç§æ–¹æ³•éƒ½æœ‰è‡ªå·±çš„æŒ‘æˆ˜ï¼Œå¯èƒ½ä¼šå½±å“æŸ¥è¯¢æ€§èƒ½ï¼š

- Pre-filteringï¼šåœ¨å‘é‡æœç´¢ä¹‹å‰è¿›è¡Œå…ƒæ•°æ®è¿‡æ»¤ã€‚è™½ç„¶è¿™å¯ä»¥å¸®åŠ©å‡å°‘æœç´¢ç©ºé—´ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´ç³»ç»Ÿå¿½ç•¥ä¸å…ƒæ•°æ®ç­›é€‰æ ‡å‡†ä¸åŒ¹é…çš„ç›¸å…³ç»“æœã€‚
- Post-filteringï¼šåœ¨å‘é‡æœç´¢å®Œæˆåè¿›è¡Œå…ƒæ•°æ®è¿‡æ»¤ã€‚è¿™å¯ä»¥ç¡®ä¿è€ƒè™‘æ‰€æœ‰ç›¸å…³ç»“æœï¼Œåœ¨æœç´¢å®Œæˆåå°†ä¸ç›¸å…³çš„ç»“æœè¿›è¡Œç­›é€‰ã€‚
- ä¸ºäº†ä¼˜åŒ–è¿‡æ»¤æµç¨‹ï¼Œå‘é‡æ•°æ®åº“ä½¿ç”¨å„ç§æŠ€æœ¯ï¼Œä¾‹å¦‚åˆ©ç”¨å…ˆè¿›çš„ç´¢å¼•æ–¹æ³•æ¥å¤„ç†å…ƒæ•°æ®æˆ–ä½¿ç”¨å¹¶è¡Œå¤„ç†æ¥åŠ é€Ÿè¿‡æ»¤ä»»åŠ¡ã€‚å¹³è¡¡æœç´¢æ€§èƒ½å’Œç­›é€‰ç²¾åº¦ä¹‹é—´çš„æƒè¡¡å¯¹äºæä¾›é«˜æ•ˆä¸”ç›¸å…³çš„å‘é‡æ•°æ®åº“æŸ¥è¯¢ç»“æœè‡³å…³é‡è¦ã€‚

### 5 å‘é‡æ•°æ®åº“

ä¸€äº›å‘é‡æ•°æ®åº“å¯¹æ¯”ï¼ˆæ•°æ®æˆªæ­¢åˆ°2024.06ä¸­æ—¬ï¼‰ã€‚

| å‘é‡æ•°æ®åº“ | URL                                    | GitHub Star | Language      | Cloud |
| ---------- | -------------------------------------- | ----------- | ------------- | ----- |
| chroma     | https://github.com/chroma-core/chroma  | 13.2K       | Python        | âŒ     |
| milvus     | https://github.com/milvus-io/milvus    | 27.9K       | Go/Python/C++ | âœ…     |
| pinecone   | https://www.pinecone.io/               | âŒ           | âŒ             | âœ…     |
| qdrant     | https://github.com/qdrant/qdrant       | 18.6K       | Rust          | âœ…     |
| typesense  | https://github.com/typesense/typesense | 18.5K       | C++           | âŒ     |
| weaviate   | https://github.com/weaviate/weaviate   | 10.1K       | Go            | âœ…     |

#### 3.5.1 qdrant

QDrantå‘é‡æ•°æ®åº“å¯ä»¥ç”¨dockerå¿«é€Ÿå®‰è£…ã€éƒ¨ç½²ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼ˆ`$(pwd)/qdrant_storage`æŒ‡æ•°æ®çš„å­˜å‚¨è·¯å¾„ï¼Œå¯ä»¥æ›¿æ¢ä¸ºè‡ªå·±å…·ä½“æœŸæœ›çš„ç›®å½•ï¼‰ï¼š

```shell
docker pull qdrant/qdrant
docker run -p 6333:6333 -p 6334:6334 \
    -v $(pwd)/qdrant_storage:/qdrant/storage:z \
    qdrant/qdrant
```

ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥é›†æˆä¾èµ–(æ”¯æŒpython/ts/rust/java/c#)ï¼Œæˆ–è€…ç”¨RestfulAPIçš„æ–¹å¼ï¼Œè¿›è¡Œæ•°æ®åº“çš„å¢åˆ æ”¹æŸ¥å·¥ä½œã€‚è¿™æ˜¯æ–‡æ¡£åœ°å€ï¼ˆå†…å«ä½¿ç”¨ç¤ºä¾‹ï¼‰ã€‚

* é›†æˆä¾èµ–: [https://qdrant.tech/documentation/quick-start/](https://qdrant.tech/documentation/quick-start/)
    * å…¶ä¸­Javaçš„ä¾èµ–ï¼š[https://github.com/qdrant/java-client.git](https://github.com/qdrant/java-client.git)
* Restful API: [https:/.qdrant.tech-reference](https:/.qdrant.tech-reference)

**QDrant Client Libraries**

| Client                                               | Repository                                            | Installation                                                 |
| :--------------------------------------------------- | :---------------------------------------------------- | :----------------------------------------------------------- |
|  ![å›¾ç‰‡](/images/6/7e22de01-757d-4ba2-88b2-cbf819b4293b.png)| **[Python](https://github.com/qdrant/qdrant-client)** | `pip install qdrant-client`                                  |
| ![å›¾ç‰‡](/images/6/ef58e413-a0f9-43f2-850e-f7e1b03a917e.png) | **[Typescript](https://github.com/qdrant/qdrant-js)** | `npm install @qdrant/js-client-rest`                         |
| ![å›¾ç‰‡](/images/6/f475677e-2a20-4743-8be3-499db57034c7.png)    | **[Rust](https://github.com/qdrant/rust-client)**     | `cargo add qdrant-client`                                    |
| ![å›¾ç‰‡](/images/6/b85ef26a-c014-49f9-b4bf-f7954aff7ced.png)    | **[Go](https://github.com/qdrant/go-client)**         | `go get github.com/qdrant/go-client`                         |
| ![å›¾ç‰‡](/images/6/1047b773-e65a-4eec-a25c-75cd2d91e690.png)  | **[.NET](https://github.com/qdrant/qdrant-dotnet)**   | `dotnet add package Qdrant.Client`                           |
| ![å›¾ç‰‡](/images/6/2ae21e4e-9bd5-4ab6-9361-a0fe41cacaa7.png)    | **[Java](https://github.com/qdrant/java-client)**     | [Available on Maven Central](https://central.sonatype.com/artifact/io.qdrant/client) |

#### 3.5.2 chroma

pythonçš„æ–°å¢ã€æŸ¥è¯¢æ•°æ®å¦‚ä¸‹ç¤ºä¾‹ã€‚æƒ³è¦äº†è§£æ›´å¤šå¯ä»¥å‚è€ƒæ–‡æ¡£: [https://docs.trychroma.com/getting-started](https://docs.trychroma.com/getting-started)

```
# my_chroma_db = MyChromaDB(â€˜./data/my_image_chroma/â€™, 'my_images')
class MyChromaDB:
    def __init__(self, db_path, collection_name):
        # æŒ‡å®š Chroma çš„æ•°æ®åº“æ–‡ä»¶å­˜å‚¨è·¯å¾„ï¼Œå¦‚æœæ•°æ®å­˜åœ¨ï¼Œç¨‹åºå¯åŠ¨çš„æ—¶å€™ä¼šè‡ªåŠ¨åŠ è½½æ•°æ®åº“æ–‡ä»¶ã€‚
        self.client = chromadb.PersistentClient(path=db_path)
        self.collection = self.client.get_or_create_collection(name=collection_name)

    # æ–°å¢æ•°æ®
    def add_data(self, image: ImageRecord):
        self.collection.add(
            ids=[str(image.table_id)],
            embeddings=[image.embeddings],
            documents=[image.filename],
        )

    # æ ¹æ®idæŸ¥è¯¢æ•°æ®
    def get_by_id(self, image_id):
        return self.collection.get(ids=[image_id], include=["embeddings"])

    # æŸ¥è¯¢å‘é‡
    def query(self, embeddings, count_limit):
        return self.collection.query(query_embeddings=embeddings, n_results=count_limit)
```

**ChromaDB Language Clients**

| language      | client                                                       |
| ------------- | ------------------------------------------------------------ |
| Python        | âœ… [`chromadb`](https://pypistats.org/packages/chromadb) (by Chroma) |
| Javascript    | âœ… [`chromadb`](https://www.npmjs.com/package/chromadb) (by Chroma) |
| Ruby          | âœ… [from @mariochavez](https://github.com/mariochavez/chroma) |
| Java          | âœ… [from @t_azarov](https://github.com/amikos-tech/chromadb-java-client) |
| Go            | âœ… [from @t_azarov](https://github.com/amikos-tech/chroma-go) |
| C#            | âœ… [from @microsoft](https://github.com/microsoft/semantic-kernel/tree/main/dotnet/src/Connectors/Connectors.Memory.Chroma) |
| Rust          | âœ… [from @Anush008](https://crates.io/crates/chromadb)        |
| Elixir        | âœ… [from @3zcurdia](https://hex.pm/packages/chroma/)          |
| Dart          | âœ… [from @davidmigloz](https://pub.dev/packages/chromadb)     |
| PHP           | âœ… [from @CodeWithKyrian](https://github.com/CodeWithKyrian/chromadb-php) |
| PHP (Laravel) | âœ… [from @HelgeSverre](https://github.com/helgeSverre/chromadb) |

## äº” æŠ€æœ¯åŸç†ä¹‹Transformerç¯‡

### 1 ç®€ä»‹

Transformer æ¨¡å‹ç”± Vaswani ç­‰äººåœ¨ 2017 å¹´æå‡ºï¼Œå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸã€‚ä¸ä¼ ç»Ÿçš„å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸åŒï¼ŒTransformer æ¨¡å‹å®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰ï¼Œå› æ­¤èƒ½å¤Ÿæ›´é«˜æ•ˆåœ°å¤„ç†é•¿åºåˆ—æ•°æ®ã€‚

Transformer æ¨¡å‹ç”±ç¼–ç å™¨ï¼ˆEncoderï¼‰å’Œè§£ç å™¨ï¼ˆDecoderï¼‰ç»„æˆã€‚ç¼–ç å™¨å’Œè§£ç å™¨å„è‡ªç”±å¤šä¸ªç›¸åŒçš„å±‚ï¼ˆLayerï¼‰å †å è€Œæˆã€‚æœ¬æ–‡å°†è¯¦ç»†ä»‹ç» Transformer æ¨¡å‹çš„Encoderã€Decoderã€‚


![å›¾ç‰‡](/images/6/7959932e-c67d-4ba1-976c-7391bccdc723.png)


### 2 Encoder

Transformerçš„Encoderéƒ¨åˆ†ï¼Œå…³é”®æ­¥éª¤å¯ä»¥ç®€åŒ–ä¸ºä¸‹è¿°å››ä¸ªæ­¥éª¤ï¼š

* Word Embeddingï¼š å°†è¯æ±‡è½¬ä¸ºæœºå™¨è®¤è¯†çš„å‘é‡ï¼ŒåŒæ—¶ä¿ç•™è¯æ±‡ä¹‹é—´çš„å…³ç³»ã€‚
* Positional Encodingï¼šå¼•å…¥è¯æ±‡çš„ä½ç½®ä¿¡æ¯ï¼Œä»è€Œèƒ½åŒºåˆ†ã€è®¤è¯†å¥å­ã€‚
* Self-Attentionï¼šå¼•å…¥å¥å­ä¸­è¯æ±‡é—´çš„ä¾èµ–ä¿¡æ¯ï¼Œä»è€Œèƒ½å¾—çŸ¥è¯æ±‡é—´çš„ä»£æŒ‡å…³ç³»ç­‰ä¿¡æ¯ã€‚
* Residual Connectionsï¼šæ®‹å·®è¿æ¥æœ‰åŠ©äºè®­ç»ƒï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±ã€‚

![å›¾ç‰‡](/images/6/2581d936-d516-47d8-b018-6505d743918e.png)


#### 2.1 Word Embedding

Word Embeddingæ˜¯ä¸€ç§å°†è¯è¯­æˆ–çŸ­è¯­ä»è¯æ±‡è¡¨ä¸­æ˜ å°„åˆ°å‘é‡ç©ºé—´çš„æŠ€æœ¯ã€‚è¿™ç§æ˜ å°„æ˜¯é€šè¿‡è®­ç»ƒå¤§é‡æ–‡æœ¬æ•°æ®å­¦ä¹ å¾—åˆ°çš„ï¼Œè®­ç»ƒåçš„è¯å‘é‡å¯ä»¥æ•æ‰åˆ°è¯è¯­ä¹‹é—´çš„è¯­ä¹‰å’Œè¯­æ³•å…³ç³»ï¼ˆç™½è¯çš„æ¦‚è¿°ï¼Œå¯å‚è€ƒï¼šç¬¬å››ç«  æŠ€æœ¯åŸç†ä¹‹å‘é‡ç¯‡çš„ç¬¬1éƒ¨åˆ†å‘é‡ä¸ç‰¹å¾ï¼‰ã€‚å‘é‡çš„æ¯ä¸ªç»´åº¦ä»£è¡¨ä¸€ä¸ªåˆ†ç±»ç‰¹å¾ï¼Œå¯ä»¥è¿›è¡Œèšåˆåˆ†ç±»ã€‚å¦‚ä¸‹å›¾ï¼Œä»£è¡¨ç®€åŒ–çš„äºŒç»´ï¼Œæ¥è¿‘çš„äº‹ç‰©æœŸæœ›è¢«åˆ†é…åˆ°å‘é‡ç›¸ä¼¼åº¦æ¥è¿‘çš„ä½ç½®ï¼š

![å›¾ç‰‡](/images/6/1333ff8c-8bf9-4014-8aaf-df238245a510.png)


å…¶ä¸­ï¼Œå¦‚ä½•æ„å»ºå‡ºWord Embeddingå‘¢ï¼Ÿè¿™é‡Œä»¥Word2Vecä¸ºä¾‹ï¼Œåœ¨æ•°æ®é¢„å¤„ç†ä¹‹åï¼Œä¼šå¼€å§‹è®­ç»ƒã€‚åœ¨è®­ç»ƒå¼€å§‹æ—¶ï¼Œä¼šä¸ºæ¯ä¸ªè¯éšæœºåˆå§‹åŒ–ä¸€ä¸ªå‘é‡ã€‚è¿™äº›å‘é‡çš„ç»´åº¦æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œè°ƒæ•´ã€‚Word2Vecæœ‰ä¸¤ç§ä¸»è¦çš„è®­ç»ƒæ¨¡å‹ç”¨æ¥è°ƒæ•´æœ€ç»ˆå‘é‡ï¼š

* Skip-gramï¼šç®—æ³•è¯•å›¾é¢„æµ‹ç»™å®šè¯å‘¨å›´çš„ä¸Šä¸‹æ–‡è¯ã€‚ç»™å®šä¸€ä¸ªè¯åï¼Œå…¶ç›®æ ‡æ˜¯æœ€å¤§åŒ–å‘¨å›´ä¸Šä¸‹æ–‡è¯çš„æ¡ä»¶æ¦‚ç‡ã€‚é€šè¿‡ä¼˜åŒ–æŸå¤±å‡½æ•°ï¼ˆå¦‚è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼‰ï¼Œä¸æ–­è°ƒæ•´è¯å‘é‡ä»¥å‡å°é¢„æµ‹è¯¯å·®ã€‚
* CBOWï¼ˆContinuous Bag of Wordsï¼‰ï¼šç®—æ³•è¯•å›¾æ ¹æ®ä¸Šä¸‹æ–‡è¯é¢„æµ‹ç»™å®šè¯ã€‚ç»™å®šä¸Šä¸‹æ–‡è¯åï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–ä¸­å¿ƒè¯çš„æ¡ä»¶æ¦‚ç‡ã€‚åŒæ ·é€šè¿‡ä¼˜åŒ–æŸå¤±å‡½æ•°æ¥è®­ç»ƒè¯å‘é‡ã€‚

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥é€šè¿‡ä¸€äº›è¯„ä¼°æŒ‡æ ‡æ¥æ£€æŸ¥è¯å‘é‡çš„è´¨é‡ã€‚å¦‚ç›¸ä¼¼åº¦æµ‹è¯•ï¼Œå¯ä»¥é€šè¿‡è®¡ç®—è¯è¯­ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦æ¥è¯„ä¼°æ¨¡å‹çš„æ•ˆæœã€‚æˆ–è€…ç”¨ç±»æ¯”æ¨ç†æœ€è¿‘é‚»è¯ï¼ŒæŸ¥æ‰¾ä¸æŸä¸ªè¯æœ€ç›¸ä¼¼çš„è¯ã€‚å¦‚æœæ•ˆæœä¸ä½³ï¼Œå¯ä»¥è°ƒæ•´è¶…å‚æ•°æˆ–æ”¹è¿›è®­ç»ƒæ•°æ®ã€‚

è®­ç»ƒå¥½çš„ Word2Vec æ¨¡å‹ï¼Œå¯ä»¥å°†è¯å‘é‡ç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€æœºå™¨ç¿»è¯‘ç­‰ã€‚è¿™äº›è¯å‘é‡èƒ½å¤Ÿæ•æ‰åˆ°è¯è¯­ä¹‹é—´çš„æ½œåœ¨å…³ç³»ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚

#### 2.2 Positional Encoding

Word Embedding èƒ½å¤Ÿæ•æ‰åˆ°è¯è¯­ä¹‹é—´çš„æ½œåœ¨å…³ç³»ï¼Œä¹Ÿå°±æ˜¯èƒ½å¤Ÿè¯»æ‡‚è¯æ±‡ã€‚ä½†æ˜¯å¯¹äºä¸€ä¸ªå¥å­æ¥è¯´ï¼Œè¯æ±‡çš„é¡ºåºå¹¶æ²¡æœ‰è¢« Word Embedding è¡¨è¾¾å‡ºæ¥ã€‚å¦‚æœæƒ³è®©æœºå™¨è¯»æ‡‚ä¸€ä¸ªå¥å­ï¼Œè¿˜éœ€è¦è¯æ±‡çš„ä½ç½®ä¿¡æ¯ã€‚

Positional Encoding æ˜¯ä¸€ç§å°†ä½ç½®ä¿¡æ¯ç¼–ç åˆ°è¯å‘é‡ä¸­çš„æ–¹æ³•ã€‚å®ƒé€šè¿‡ä¸ºæ¯ä¸ªä½ç½®ç”Ÿæˆä¸€ä¸ªå›ºå®šçš„å‘é‡ï¼Œå¹¶å°†å…¶ä¸è¯å‘é‡ç›¸åŠ ï¼Œä»è€Œä¸ºæ¨¡å‹æä¾›ä½ç½®ä¿¡æ¯ã€‚

è¿™é‡ŒTransformeré€šè¿‡é¢„å®šä¹‰ä½ç½®è®¡ç®—å‡½æ•°æ¥å®ç°çš„ï¼Œå…·ä½“å®ç°é‡‡ç”¨çš„æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ã€‚å¯¹äºåºåˆ—ä¸­çš„ç¬¬ ( pos ) ä¸ªä½ç½®å’Œè¯å‘é‡çš„ç¬¬ ( i ) ä¸ªç»´åº¦ï¼ŒPositional Encoding çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼š

å¯¹äºä½ç½® `(pos) `å’Œç»´åº¦` (i)`ï¼ŒPositional Encoding çš„è®¡ç®—å…¬å¼å¦‚ä¸‹ï¼ˆæ­£å¼¦å’Œä½™å¼¦å‡½æ•°çš„å‘¨æœŸæ€§ä½¿å¾—ä¸åŒä½ç½®çš„ç¼–ç å…·æœ‰ç‹¬ç‰¹æ€§ï¼Œè€Œé€šè¿‡ä¸åŒé¢‘ç‡çš„æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ï¼Œå¯ä»¥æ•æ‰åˆ°ä¸åŒç²’åº¦çš„ä½ç½®ä¿¡æ¯ï¼‰ï¼š
$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)
$$

$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)
$$

å…¶ä¸­ï¼š

- `pos`æ˜¯ä½ç½®ç´¢å¼•ã€‚
- `i`æ˜¯ç»´åº¦ç´¢å¼•ã€‚
- `d` æ˜¯æ¨¡å‹çš„ç»´åº¦ã€‚

ä¹‹åï¼ŒPosition Encoding ä¼šä¸ Word Embedding ç›¸åŠ ï¼ˆPosition Encodingæ‹¼æ¥Word Embeddingçš„è¯ï¼Œä¼šå°†ç»´åº¦å¢åŠ ï¼‰ã€‚å¦‚ä¸‹ç¤ºä¾‹ï¼š

![å›¾ç‰‡](/images/6/29e54642-5fb0-477c-8fcf-401845c76628.png)


#### 2.3 Self-Attention

Self-Attention æ˜¯ä¸€ç§ç‰¹æ®Šçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒåœ¨åŒä¸€ä¸ªåºåˆ—ä¸­è®¡ç®—æ¯ä¸ªä½ç½®ä¸å…¶ä»–ä½ç½®çš„ç›¸å…³æ€§ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨ç¼–ç æˆ–è§£ç è¿‡ç¨‹ä¸­åŒæ—¶å…³æ³¨åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®ï¼Œä»è€Œæ•æ‰åˆ°å…¨å±€çš„ä¾èµ–å…³ç³»ã€‚æ¯”å¦‚ä¸‹é¢è¿™å¥è¯ï¼Œåˆ©ç”¨å…¶å¯ä»¥åˆ¤æ–­â€œitâ€å¯¹å…¶ä»–è¯æ±‡çš„ä¾èµ–å…³ç³»ï¼Œå¾—åˆ°â€œpizzaâ€å¯¹â€œitâ€çš„å½±å“å¯¹å¤§ï¼Œä»è€Œå¾—çŸ¥è¿™é‡Œçš„â€œitâ€ä»£æŒ‡â€œpizzaâ€(è€Œä¸æ˜¯â€œovenâ€)ã€‚


![å›¾ç‰‡](/images/6/698361ac-f225-49ae-93b6-2d8a6de127c3.png)


Self-Attention çš„è®¡ç®—å…¬å¼å¯ä»¥è¡¨ç¤ºä¸º
$$
Attention(ğ‘„,ğ¾,ğ‘‰) = \text{Softmax}\left(\frac{QK^T} {\sqrt{d_k}}\right)\text{V}
$$
å…¶å…·ä½“æ­¥éª¤æ˜¯ï¼Œå¯¹äºè¾“å…¥ä¸€å¥è¯å„ä¸ªè¯æ±‡çš„åµŒå…¥æ•°æ®ï¼Œåˆ†åˆ«é€šè¿‡ä¸‰ä¸ªä¸åŒçš„çº¿æ€§å˜æ¢ï¼Œå¾—åˆ°ä¸åŒè¯æ±‡çš„æŸ¥è¯¢Qï¼ˆQueryï¼‰ã€é”®Kï¼ˆKeyï¼‰å’Œå€¼Vï¼ˆValueï¼‰çŸ©é˜µã€‚ç„¶åå¯¹äºæŸä¸ªè¯æ±‡ï¼Œè®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼Œè¿‡ç¨‹æ˜¯ç»“åˆå…¶ä»–è¯æ±‡çš„Kè·Ÿè‡ªå·±çš„Qï¼Œå¾—åˆ°æ–°çš„å€¼ã€‚ç„¶åå¯¹äºKè·ŸQå¾—åˆ°çš„æ–°çš„å€¼ï¼Œé€šè¿‡SoltMaxå½’ä¸€åŒ–å¾—åˆ°æ³¨æ„åŠ›æƒé‡ã€‚æœ€ç»ˆä½¿ç”¨æ³¨æ„åŠ›æƒé‡å¯¹å€¼çŸ©é˜µ ( V ) è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå¾—åˆ°æœ€ç»ˆçš„ Self-Attention è¾“å‡ºã€‚

![å›¾ç‰‡](/images/6/16dc404f-0a42-43e0-80a5-f5ce595a7ec1.png)


ä¸ºäº†å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼ŒTransformer å¼•å…¥äº†å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Self-Attentionï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œå°†æŸ¥è¯¢ã€é”®å’Œå€¼çŸ©é˜µåˆ†æˆå¤šä¸ªå¤´ï¼ˆheadï¼‰ï¼Œæ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›ï¼Œç„¶åå°†æ‰€æœ‰å¤´çš„è¾“å‡ºæ‹¼æ¥èµ·æ¥ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢å¾—åˆ°æœ€ç»ˆè¾“å‡ºã€‚

åœ¨ Transformer æ¨¡å‹ä¸­ï¼ŒSelf-Attention æœºåˆ¶è¢«å¹¿æ³›åº”ç”¨äºç¼–ç å™¨å’Œè§£ç å™¨çš„æ¯ä¸€å±‚ã€‚ç¼–ç å™¨ä¸­çš„ Self-Attention ä½¿å¾—æ¯ä¸ªè¯èƒ½å¤Ÿå…³æ³¨è¾“å…¥åºåˆ—ä¸­çš„æ‰€æœ‰å…¶ä»–è¯ï¼Œè€Œè§£ç å™¨ä¸­çš„ Self-Attention åˆ™ä½¿å¾—æ¯ä¸ªè¯èƒ½å¤Ÿå…³æ³¨è§£ç å™¨è¾“å‡ºåºåˆ—ä¸­çš„æ‰€æœ‰å…¶ä»–è¯ã€‚

#### 2.4 Residual Connections

Residual Connections æ˜¯ä¸€ç§åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­å¼•å…¥çš„æŠ€æœ¯ï¼Œæœ€æ—©ç”± He ç­‰äººåœ¨ ResNetï¼ˆResidual Networksï¼‰ä¸­æå‡ºã€‚å®ƒé€šè¿‡åœ¨å±‚ä¸å±‚ä¹‹é—´æ·»åŠ ç›´æ¥çš„è·³è·ƒè¿æ¥ï¼ˆskip connectionsï¼‰ï¼Œä½¿å¾—ä¿¡æ¯èƒ½å¤Ÿæ›´å®¹æ˜“åœ°åœ¨ç½‘ç»œä¸­ä¼ æ’­ï¼Œä»è€Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ã€‚

å…³äºå…¶è®¡ç®—å…¬å¼ã€‚å‡è®¾æŸä¸€å±‚çš„è¾“å…¥ä¸º ( x )ï¼Œç»è¿‡ä¸€ç³»åˆ—å˜æ¢ï¼ˆå¦‚çº¿æ€§å˜æ¢å’Œæ¿€æ´»å‡½æ•°ï¼‰åçš„è¾“å‡ºä¸º ( F(x) )ã€‚åœ¨å¼•å…¥ Residual Connections åï¼Œè¯¥å±‚çš„è¾“å‡ºå˜ä¸ºï¼š ğ‘¦=ğ¹(ğ‘¥)+ğ‘¥ å…¶ä¸­ï¼Œ( F(x) ) è¡¨ç¤ºè¯¥å±‚çš„å˜æ¢å‡½æ•°ï¼Œ( x ) è¡¨ç¤ºè¾“å…¥ã€‚

åœ¨ Transformer æ¨¡å‹ä¸­ï¼ŒResidual Connections è¢«å¹¿æ³›åº”ç”¨äºç¼–ç å™¨å’Œè§£ç å™¨çš„æ¯ä¸€å±‚ã€‚å…·ä½“æ¥è¯´ï¼Œæ¯ä¸€å±‚çš„è¾“å‡ºä¸ä»…åŒ…å«å½“å‰å±‚çš„å˜æ¢ç»“æœï¼Œè¿˜åŒ…å«ä¸Šä¸€å±‚çš„è¾“å…¥ä¿¡æ¯ã€‚

#### 2.5 Transformer å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Neural Networkï¼‰

åœ¨ä¸Šé¢2.1ï½2.4æ­¥éª¤ä¸­ï¼Œæ¯ä¸ªä½ç½®çš„ç‰¹å¾å‘é‡æ˜¯é€šè¿‡ä¸å…¶ä»–ä½ç½®çš„ç‰¹å¾å‘é‡çš„åŠ æƒå’ŒçŸ©é˜µè¿ç®—æ¥è®¡ç®—å¾—åˆ°çš„ï¼Œè¿™ç§å˜åŒ–å¯èƒ½éš¾ä»¥æ•æ‰åˆ°å¤æ‚çš„éçº¿æ€§å…³ç³»ã€‚

å‰é¦ˆç¥ç»ç½‘ç»œï¼Œæ˜¯ä¸€ç§å•å‘ä¼ è¾“çš„å¤šå±‚ç»“æ„ç½‘ç»œï¼Œç”±ä¸€ä¸ªæˆ–å¤šä¸ªçº¿æ€§å˜æ¢å’Œéçº¿æ€§æ¿€æ´»å‡½æ•°ç»„æˆã€‚

åœ¨Transformerä¸­ï¼Œå‰é¦ˆç¥ç»ç½‘ç»œé€šè¿‡ä¸¤ä¸ªçº¿æ€§å˜æ¢å’Œä¸€ä¸ªéçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œå¯¹æ¯ä¸ªä½ç½®çš„è¡¨ç¤ºè¿›è¡Œç‹¬ç«‹çš„å˜æ¢ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•æ‰åˆ°æ›´å¤æ‚çš„ç‰¹å¾ï¼Œä»è€Œæå‡å…¶è¡¨è¾¾èƒ½åŠ›ã€‚

### 3 Decoder

#### 3.1 è‡ªæ³¨æ„åŠ›æœºåˆ¶

è¿™é‡Œä¸»è¦æŒ‡ä»¥ä¸‹ä¸¤ç§æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¸å†èµ˜è¿°ï¼š

**æ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼šæ•æ‰è§£ç å™¨è¾“å‡ºåºåˆ—ä¸­çš„ä¾èµ–å…³ç³»ã€‚

**ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›æœºåˆ¶**ï¼šæ•æ‰ç¼–ç å™¨è¾“å‡ºä¸è§£ç å™¨è¾“å…¥ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚

![å›¾ç‰‡](/images/6/1bce83e5-3e9d-4a5c-a4d3-2ea6479c8afa.png)


#### 3.2 å‰é¦ˆç¥ç»ç½‘ç»œ

è¿™é‡Œç±»ä¼¼ 2.5 å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Neural Networkï¼‰ï¼Œå¯¹æ¯ä¸ªä½ç½®çš„è¡¨ç¤ºè¿›è¡Œç‹¬ç«‹çš„éçº¿æ€§å˜æ¢ã€‚

#### 3.3 å…¨è¿æ¥å±‚

è¿™é‡Œå¼•å…¥å…¨è¿æ¥å±‚ï¼Œä»¥è®¡ç®—å‡ºæ­¤æ­¥éª¤æœ€ç»ˆè¦ç¿»è¯‘å‡ºçš„ç»“æœæ˜¯å“ªä¸ªè¯æ±‡ã€‚

![å›¾ç‰‡](/images/6/7d6a3617-b144-467d-816f-bc485973f2c3.png)


#### 3.4 è®¡ç®—å„ä¸ªè¯æ±‡å¾—åˆ°æ•´ä¸ªå¥å­

ä»¥â€œlet's go.â€ç¿»è¯‘æˆè¥¿ç­ç‰™è¯­ä¸ºä¾‹ï¼Œä¼šå…ˆç”±å¥å­ç»“å°¾EOSå¾—åˆ°vamosï¼Œä¹‹åä¼šç”±vamoså¼€å§‹ç»§ç»­ç¿»è¯‘ä¸‹ä¸€ä¸ªä½ç½®çš„è¯æ±‡ï¼Œå¾—åˆ°EOSçš„æ¦‚ç‡æœ€å¤§ã€‚æœ€ç»ˆï¼Œâ€œlet's go.â€ç¿»è¯‘æˆäº†â€œvamos.â€ã€‚

![å›¾ç‰‡](/images/6/402d1a72-5231-4323-8fd0-031f4fb4bbc6.png)


## å…­ æŠ€æœ¯åŸç†ä¹‹Vision Transfomerç¯‡

### 1 ç®€ä»‹

åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼ŒVision Transformer (ViT) æ˜¯ä¸€ç§æ–°å…´çš„æ¨¡å‹æ¶æ„ï¼Œå®ƒå°† Transformer æ¨¡å‹çš„å¼ºå¤§èƒ½åŠ›å¼•å…¥åˆ°å›¾åƒå¤„ç†ä»»åŠ¡ä¸­ã€‚ViT ç”± Google Research å›¢é˜Ÿåœ¨ 2020 å¹´æå‡ºï¼Œå¹¶åœ¨å¤šä¸ªè§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æœ¬æ–‡å°†è¯¦ç»†ä»‹ç» Vision Transformer çš„åŸºæœ¬ç»“æ„ã€æ ¸å¿ƒç»„ä»¶åŠå…¶åº”ç”¨ã€‚

![å›¾ç‰‡](/images/6/547b4a52-32c8-4770-bb3f-e9d145b2740b.png)


Transformer æ¨¡å‹é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attention Mechanismï¼‰ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°æ•æ‰åºåˆ—ä¸­çš„å…¨å±€ä¾èµ–å…³ç³»ã€‚ViT å°†è¿™ä¸€ä¼˜åŠ¿å¼•å…¥åˆ°å›¾åƒå¤„ç†ä»»åŠ¡ä¸­ï¼Œä»è€Œå…‹æœäº† CNN çš„ä¸€äº›å±€é™æ€§ï¼ˆå±€éƒ¨å·ç§¯æ ¸æ•æ‰å›¾åƒç‰¹å¾ï¼Œéš¾ä»¥æ•æ‰å…¨å±€ä¾èµ–å…³ç³»ï¼›å¤šå±‚å·ç§¯é€æ­¥æå–é«˜å±‚æ¬¡ç‰¹å¾ï¼Œå¯èƒ½å¯¼è‡´ä¿¡æ¯ä¸¢å¤±ï¼‰ã€‚

ViT çš„åŸºæœ¬ç»“æ„ä¸åŸå§‹ Transformer æ¨¡å‹ç±»ä¼¼ï¼Œä½†åœ¨è¾“å…¥å¤„ç†å’Œä½ç½®ç¼–ç ä¸Šæœ‰æ‰€ä¸åŒã€‚ViT çš„ä¸»è¦æ­¥éª¤åŒ…æ‹¬å›¾åƒåˆ†å—ã€çº¿æ€§åµŒå…¥ã€ä½ç½®ç¼–ç ã€Transformer ç¼–ç å™¨ç­‰ã€‚

### 2 æ­¥éª¤
è¿™é‡Œç®€å•ä»‹ç»ViTçš„ä¸€äº›æ­¥éª¤ï¼ŒåŠ¨æ€æµç¨‹å¯ä»¥å‚è€ƒæ­¤åŠ¨å›¾ã€‚

![å›¾ç‰‡](/images/6/4b739e75-9b82-4c61-84ac-92c85df03832.gif)

#### 2.1 å›¾åƒåˆ†å—(Patch Embedding) & çº¿æ€§åµŒå…¥(Linear Embedding)

å¯¹äºå›¾åƒæ•°æ®è€Œè¨€ï¼Œå…¶æ•°æ®æ ¼å¼ä¸º[H, W, C]æ˜¯ä¸‰ç»´çŸ©é˜µæ˜æ˜¾ä¸æ˜¯Transformeræƒ³è¦çš„ã€‚æ‰€ä»¥éœ€è¦å…ˆé€šè¿‡ä¸€ä¸ªEmbeddingå±‚æ¥å¯¹æ•°æ®åšä¸ªå˜æ¢ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œé¦–å…ˆå°†ä¸€å¼ å›¾ç‰‡æŒ‰ç»™å®šå¤§å°åˆ†æˆä¸€å †Patchesã€‚ä»¥ViT-B/16ä¸ºä¾‹ï¼Œå°†è¾“å…¥å›¾ç‰‡(224x224)æŒ‰ç…§16x16å¤§å°çš„Patchè¿›è¡Œåˆ’åˆ†ï¼Œåˆ’åˆ†åä¼šå¾—åˆ° **(224/16)^2=14^2 =196**ä¸ªPatchesã€‚æ¥ç€é€šè¿‡çº¿æ€§æ˜ å°„å°†æ¯ä¸ªPatchæ˜ å°„åˆ°ä¸€ç»´å‘é‡ä¸­ï¼Œä»¥ViT-B/16ä¸ºä¾‹ï¼Œæ¯ä¸ªPatcheæ•°æ®shapeä¸º[16, 16, 3]é€šè¿‡æ˜ å°„å¾—åˆ°ä¸€ä¸ªé•¿åº¦ä¸º768çš„å‘é‡ï¼ˆåé¢éƒ½ç›´æ¥ç§°ä¸ºtokenï¼‰ã€‚**[16, 16, 3] -> [768]**

åœ¨ä»£ç å®ç°ä¸­ï¼Œç›´æ¥é€šè¿‡ä¸€ä¸ªå·ç§¯å±‚æ¥å®ç°ã€‚ ä»¥ViT-B/16ä¸ºä¾‹ï¼Œç›´æ¥ä½¿ç”¨ä¸€ä¸ªå·ç§¯æ ¸å¤§å°ä¸º16x16ï¼Œæ­¥è·ä¸º16ï¼Œå·ç§¯æ ¸ä¸ªæ•°ä¸º768çš„å·ç§¯æ¥å®ç°ã€‚é€šè¿‡å·ç§¯**[224, 224, 3]** -> **[14, 14, 768]**ï¼Œç„¶åæŠŠHä»¥åŠWä¸¤ä¸ªç»´åº¦å±•å¹³å³å¯**[14, 14, 768] -> [196, 768]**ï¼Œæ­¤æ—¶æ­£å¥½å˜æˆäº†ä¸€ä¸ªäºŒç»´çŸ©é˜µï¼Œæ­£æ˜¯Transformeræƒ³è¦çš„ã€‚

![å›¾ç‰‡](/images/6/2987fe2c-1917-4076-8c10-7215d10e2aad.png)

![å›¾ç‰‡](/images/6/a756849d-e4c0-411e-8214-de83ad574870.png)

#### 2.2 ä½ç½®ç¼–ç (Positional Encoding) &  CLASS Token

åœ¨è¾“å…¥Transformer Encoderä¹‹å‰æ³¨æ„éœ€è¦åŠ ä¸Š[class]tokenä»¥åŠPosition Embeddingã€‚ åœ¨åŸè®ºæ–‡ä¸­ï¼Œä½œè€…è¯´å‚è€ƒBERTï¼Œåœ¨åˆšåˆšå¾—åˆ°çš„ä¸€å †tokensä¸­æ’å…¥ä¸€ä¸ªä¸“é—¨ç”¨äºåˆ†ç±»çš„[class]tokenï¼Œè¿™ä¸ª[class]tokenæ˜¯ä¸€ä¸ªå¯è®­ç»ƒçš„å‚æ•°ï¼Œæ•°æ®æ ¼å¼å’Œå…¶ä»–tokenä¸€æ ·éƒ½æ˜¯ä¸€ä¸ªå‘é‡ï¼Œä»¥ViT-B/16ä¸ºä¾‹ï¼Œå°±æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º768çš„å‘é‡ï¼Œä¸ä¹‹å‰ä»å›¾ç‰‡ä¸­ç”Ÿæˆçš„tokensæ‹¼æ¥åœ¨ä¸€èµ·ï¼Œ**Cat([1, 768], [196, 768]) -> [197, 768]**ã€‚

![å›¾ç‰‡](/images/6/24416c0c-ebf5-4bd1-b909-943ba6e5a772.png)


å…³äºPosition Embeddingå°±æ˜¯ä¹‹å‰Transformerä¸­è®²åˆ°çš„Positional Encodingï¼Œè¿™é‡Œçš„Position Embeddingé‡‡ç”¨çš„æ˜¯ä¸€ä¸ªå¯è®­ç»ƒçš„å‚æ•°ï¼ˆ*1D Pos. Emb.*ï¼‰ï¼Œæ˜¯ç›´æ¥å åŠ åœ¨tokensä¸Šçš„ï¼ˆaddï¼‰ï¼Œæ‰€ä»¥shapeè¦ä¸€æ ·ã€‚ä»¥ViT-B/16ä¸ºä¾‹ï¼Œåˆšåˆšæ‹¼æ¥[class]tokenåshapeæ˜¯[197, 768]ï¼Œé‚£ä¹ˆè¿™é‡Œçš„Position Embeddingçš„shapeä¹Ÿæ˜¯**[197, 768]**ã€‚

![å›¾ç‰‡](/images/6/979ca77c-2e48-42b3-b30a-58e0707cd884.png)

#### 2.3 Transformer ç¼–ç å™¨

å°†åµŒå…¥åçš„å›¾åƒå—åºåˆ—è¾“å…¥åˆ°å¤šä¸ª Transformer ç¼–ç å™¨å±‚ä¸­ã€‚æ¯ä¸ªç¼–ç å™¨å±‚åŒ…å«å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Self-Attentionï¼‰å’Œå‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Neural Networkï¼‰ï¼Œå¹¶åœ¨æ¯ä¸ªå­å±‚ä¹‹åæ·»åŠ æ®‹å·®è¿æ¥ï¼ˆResidual Connectionsï¼‰å’Œå±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰ã€‚

è¿™é‡Œçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è§†è§‰ä¸Šï¼Œå¯ä»¥æŒ‰ç…§ä¸‹å›¾è¿›è¡Œç›´è§‚ç†è§£ã€‚

<div align="center">
<img src="/images/6/2d7a1323-b745-4582-b3d4-f634a96b2f2a.png" width=60% />
</div>


## ä¸ƒ æŠ€æœ¯åŸç†ä¹‹clipä»£ç ç¯‡
è¿™é‡Œç®€ä»‹ï¼Œå¯ä»¥ç›´æ¥å‚è€ƒä¸‹é¢çš„å¼•ç”¨ï¼š

> **CLIP (Contrastive Language-Image Pre-Training)** is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3. We found CLIP matches the performance of the original ResNet50 on ImageNet â€œzero-shotâ€ without using any of the original 1.28M labeled examples, overcoming several major challenges in computer vision.

CLIPçš„ä¸»è¦æ­¥éª¤ï¼Œå¯ä»¥å‚è€ƒæ­¤è§†é¢‘ã€‚
!video[clip_process_demo.mp4](68359)

### 1 ä¸»å‡½æ•°

CLIPæ¨¡å‹ä¸»å‡½æ•°åœ¨æºç model.pyæ–‡ä»¶ä¸­ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

<div align="center">
<img src="/images/6/84a5d869-f216-4150-86ce-0bbc02bcd806.png" width=60% />
</div>


### 2 image encode ä»£ç 

#### 2.1 CLIPä¸»å‡½æ•°éƒ¨åˆ†

```
# è¿™é‡Œæ˜¯å¯¹å›¾åƒè¿›è¡Œç¼–ç ï¼Œå¾—åˆ°å‘é‡åŒ–æ•°æ®
def encode_image(self, image):
    return self.visual(image.type(self.dtype))
```

CLIPä½¿ç”¨å›¾åƒç¼–ç æœ‰ResNetç»“æ„ä¸VisionTransformer,å‰è€…æ˜¯CNNæ–¹å¼ï¼Œåè€…æ˜¯transformeræ–¹å¼ï¼Œæˆ‘å°†ä»¥transformeræ–¹å¼è§£è¯»ï¼Œå¦‚ä¸‹ä»£ç ï¼š

```
if isinstance(vision_layers, (tuple, list)):
    vision_heads = vision_width * 32 // 64
    self.visual = ModifiedResNet(
        layers=vision_layers,
        output_dim=embed_dim,
        heads=vision_heads,
        input_resolution=image_resolution,
        width=vision_width
    )
else:
    vision_heads = vision_width // 64
    self.visual = VisionTransformer(
        input_resolution=image_resolution,
        patch_size=vision_patch_size,
        width=vision_width,
        layers=vision_layers,
        heads=vision_heads,
        output_dim=embed_dim
    )
```

#### 2.2 ViTéƒ¨åˆ†

è¿™é‡Œçš„æ ¸å¿ƒæ­¥éª¤ï¼Œæ˜¯forwardå‡½æ•°ã€‚è¿™é‡Œåœ¨ä¸‹é¢æºç åŠ äº†æ³¨é‡Šï¼š

```
class VisionTransformer(nn.Module):
    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):
        super().__init__()
        self.input_resolution = input_resolution
        self.output_dim = output_dim
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)

        scale = width ** -0.5
        self.class_embedding = nn.Parameter(scale * torch.randn(width))
        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))
        self.ln_pre = LayerNorm(width)

        self.transformer = Transformer(width, layers, heads)

        self.ln_post = LayerNorm(width)
        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))

    def forward(self, x: torch.Tensor):
        # å¼€å§‹æ—¶ï¼Œx = [1, 3, 224, 224]
        # shape = [*, width, grid, grid], å°†å›¾ç‰‡åˆ†æˆ[32,32]ä¸ªpatchï¼Œå˜æˆ[1,768,7,7]
        x = self.conv1(x)  #
        # shape = [*, width, grid ** 2], åˆå¹¶é«˜å®½ [1,768,49]
        x = x.reshape(x.shape[0], x.shape[1], -1)
        # shape = [*, grid ** 2, width], æ›´æ¢ä½ç½® [1,49,768]
        x = x.permute(0, 2, 1)
        # shape = [*, grid ** 2 + 1, width], æ·»åŠ classTokenä¿¡æ¯ï¼Œå˜ä¸º[1,50,768]
        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)
        # å¢åŠ ä½ç½®ä¿¡æ¯
        x = x + self.positional_embedding.to(x.dtype)
        x = self.ln_pre(x)

        # NLD -> LND, ä»[1,50,768]å¾—åˆ°[50,1,768]
        x = x.permute(1, 0, 2)
        # å¤šå¤´transformerï¼Œï¼Œå¢åŠ æ³¨æ„åŠ›ä¿¡æ¯ï¼ŒçŸ©é˜µä¸å˜å¾—åˆ°x [50,1,768]
        x = self.transformer(x)
        # LND -> NLD, ä»[50,1,768]å¾—åˆ°[1,50,768]
        x = x.permute(1, 0, 2)

        # x[:, 0, :] å°†æ‰€æœ‰ä¿¡æ¯æ±‡èšåˆ°cls tokenä¸­ï¼Œåªéœ€å‰é¢æ¥åšä¸‹æ¸¸ä»»åŠ¡ [1,768]
        x = self.ln_post(x[:, 0, :])
        
        # self.projæ˜¯å¯å­¦ä¹ å‚æ•°ï¼Œç»´åº¦ä¸º[768,512]
        if self.proj is not None:
            # é€šè¿‡å­¦ä¹ å‚æ•°å°†ç»´åº¦å†æ¬¡èåˆå˜æˆ512ç‰¹å¾ï¼Œæœ€ç»ˆä¸º[1,512]
            x = x @ self.proj

        return x
```

### 3 text encode ä»£ç 

CLIPä¸»å‡½æ•°éƒ¨åˆ†å¦‚ä¸‹

```
def encode_text(self, text):
    x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]

    # å¢åŠ ä½ç½®ä¿¡æ¯
    x = x + self.positional_embedding.type(self.dtype)
    x = x.permute(1, 0, 2)  # NLD -> LND
    # å¤šå¤´transformerï¼Œå¢åŠ æ³¨æ„åŠ›ä¿¡æ¯ï¼ŒçŸ©é˜µä¸å˜
    x = self.transformer(x)
    x = x.permute(1, 0, 2)  # LND -> NLD
    x = self.ln_final(x).type(self.dtype)

    # x.shape = [batch_size, n_ctx, transformer.width]
    # take features from the eot embedding (eot_token is the highest number in each sequence)
    x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection

    return x
```

### 4 forward ä»£ç 

```
def forward(self, image, text):
    image_features = self.encode_image(image)
    text_features = self.encode_text(text)

    # normalized features
    image_features = image_features / image_features.norm(dim=1, keepdim=True)
    text_features = text_features / text_features.norm(dim=1, keepdim=True)

    # cosine similarity as logits
    logit_scale = self.logit_scale.exp()
    # ç‰¹å¾ç›¸ä¹˜è·å¾—ç›¸ä¼¼åº¦
    logits_per_image = logit_scale * image_features @ text_features.t()
    # å˜æˆæ–‡æœ¬
    logits_per_text = logits_per_image.t()

    # shape = [global_batch_size, global_batch_size]
    return logits_per_image, logits_per_text
```
## å…« å°ç»“

ç»è¿‡äº†è§£ï¼Œç°åœ¨æœ‰ä¸€äº›å¼€æºç›¸å†Œé¡¹ç›®ï¼Œä¹Ÿå·²ä½¿ç”¨CLIPæä¾›æœç´¢åŠŸèƒ½ï¼Œæ¯”å¦‚[immichd](https://github.com/immich-app/immich.git)çš„[æ™ºèƒ½æœç´¢](https://immich.app/docs/features/smart-search)ã€‚

> Smart search is powered by the [pgvecto.rs](https://github.com/tensorchord/pgvecto.rs) extension, utilizing machine learning models like [CLIP](https://openai.com/research/clip) to provide relevant search results. This allows for freeform searches without requiring specific keywords in the image or video metadata.

æœ‰å…¶ä»–å¼€å‘è€…ä¹Ÿå¯¹CLIPçš„ç¥å¥‡æ„Ÿåˆ°äº†æƒŠè®¶ï¼Œæ¯”å¦‚è¿™ä¸ªåšå®¢[I accidentally built a meme search engine](https://harper.blog/2024/04/12/i-accidentally-built-a-meme-search-engine/)ä»‹ç»çš„ï¼Œåªæœ‰10ä¸ªå°æ—¶å°±ç®€å•å®ç°äº†å›¾åƒæœç´¢å¼•æ“ã€‚

> I canâ€™t imagine the amount of work we would have done in 2015 to build this. I spent maybe 10 hours total on this and it was trivial.
>
> The results are akin to magic.

å™«ååš±ï¼Œä¸å¾—ä¸æ„Ÿå¹å¤šæ¨¡æ€æŠ€æœ¯çš„å¼ºå¤§ï¼

é’ˆå¯¹æœ¬æ–‡çš„å›¾åƒæœç´¢ï¼Œä¹Ÿå¯ä»¥æ‹“å±•æ›´å¤šçš„åŠŸèƒ½ã€‚æ¯”å¦‚æ·»åŠ å¤‡æ³¨æ–‡æœ¬ä¿¡æ¯ã€è·å–å›¾åƒæ–‡ä»¶ä¸­çš„GPSä¿¡æ¯ç­‰æä¾›filteråŠŸèƒ½ã€‚


## å‚è€ƒå¼•ç”¨
- WeCLIPï¼šå›¾æ–‡é¢„è®­ç»ƒæœ€å¼ºä¸­æ–‡CLIP æ¨¡å‹[https://km.woa.com/articles/show/592047](https://km.woa.com/articles/show/592047)
- å‘é‡æ•°æ®åº“[https://guangzhengli.com/blog/zh/vector-database/](https://guangzhengli.com/blog/zh/vector-database/)
-  å‘é‡æ•°æ®åº“æŠ€æœ¯é‰´èµï¼ˆä¸Šï¼‰[https://www.bilibili.com/video/BV11a4y1c7SW](https://www.bilibili.com/video/BV11a4y1c7SW)
- å‘é‡æ•°æ®åº“æŠ€æœ¯é‰´èµï¼ˆä¸‹ï¼‰[https://www.bilibili.com/video/BV1BM4y177Dk](https://www.bilibili.com/video/BV1BM4y177Dk)
- Chroma [https://docs.trychroma.com/getting-started](https://docs.trychroma.com/getting-started)
- QDrant [https://qdrant.tech/documentation/quick-start/](https://qdrant.tech/documentation/quick-start/)
- HNSW for Vector Search Explained and Implemented with Faiss (Python) [https://www.youtube.com/watch?v=QvKMwLjdK-s](https://www.youtube.com/watch?v=QvKMwLjdK-s)
- CLIP [https://arxiv.org/abs/2103.00020](https://arxiv.org/abs/2103.00020)
- Transformer [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
- Vision Transfomer [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)
- Transformerè®²è§£[https://youtu.be/zxQyTK8quyY?si=SC3EtcxtgnwLuhOV](https://youtu.be/zxQyTK8quyY?si=SC3EtcxtgnwLuhOV)
    * Bç«™æ¬è¿åˆé›†ï¼š [https://space.bilibili.com/3546620985608836/channel/collectiondetail?sid=2298262](https://space.bilibili.com/3546620985608836/channel/collectiondetail?sid=2298262)
- Vision Transformerè¯¦è§£ [https://blog.csdn.net/qq_37541097/article/details/118242600](https://blog.csdn.net/qq_37541097/article/details/118242600)
- å¤šæ¨¡æ€æ¨¡å‹å­¦ä¹ 1â€”â€”CLIPå¯¹æ¯”å­¦ä¹  è¯­è¨€-å›¾åƒé¢„è®­ç»ƒæ¨¡å‹ [https://blog.csdn.net/weixin_44791964/article/details/129941386](https://blog.csdn.net/weixin_44791964/article/details/129941386)
- å¤šæ¨¡æ€è¡¨å¾â€”CLIPåŠä¸­æ–‡ç‰ˆChinese-CLIPï¼šç†è®ºè®²è§£ã€ä»£ç å¾®è°ƒä¸è®ºæ–‡é˜…è¯» [https://blog.csdn.net/weixin_44362044/article/details/136262247](https://blog.csdn.net/weixin_44362044/article/details/136262247)

